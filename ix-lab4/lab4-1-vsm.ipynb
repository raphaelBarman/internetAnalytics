{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text 1: Vector space models\n",
    "**Internet Analytics - Lab 4**\n",
    "\n",
    "---\n",
    "\n",
    "**Group:** R\n",
    "**Names:**\n",
    "\n",
    "* Raphael Strebel\n",
    "* Raphaël Barman\n",
    "* Thierry Bossy\n",
    "\n",
    "---\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "*This is a template for part 1 of the lab. Clearly write your answers, comments and interpretations in Markodown cells. Don't forget that you can add $\\LaTeX$ equations in these cells. Feel free to add or remove any cell.*\n",
    "\n",
    "*Please properly comment your code. Code readability will be considered for grading. To avoid long cells of codes in the notebook, you can also embed long python functions and classes in a separate module. Don’t forget to hand in your module if that is the case. In multiple exercises, you are required to come up with your own method to solve various problems. Be creative and clearly motivate and explain your methods. Creativity and clarity will be considered for grading.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"http://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"b4359354-859e-4ac4-b8e9-9405c44e330e\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(global) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = \"1\";\n",
       "\n",
       "  if (typeof (window._bokeh_onload_callbacks) === \"undefined\" || force !== \"\") {\n",
       "    window._bokeh_onload_callbacks = [];\n",
       "    window._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "\n",
       "  \n",
       "  if (typeof (window._bokeh_timeout) === \"undefined\" || force !== \"\") {\n",
       "    window._bokeh_timeout = Date.now() + 5000;\n",
       "    window._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    if (window.Bokeh !== undefined) {\n",
       "      Bokeh.$(\"#b4359354-859e-4ac4-b8e9-9405c44e330e\").text(\"BokehJS successfully loaded.\");\n",
       "    } else if (Date.now() < window._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    window._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    delete window._bokeh_onload_callbacks\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    window._bokeh_onload_callbacks.push(callback);\n",
       "    if (window._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    window._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        window._bokeh_is_loading--;\n",
       "        if (window._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"b4359354-859e-4ac4-b8e9-9405c44e330e\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid 'b4359354-859e-4ac4-b8e9-9405c44e330e' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = ['https://cdn.pydata.org/bokeh/release/bokeh-0.12.2.min.js', 'https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.2.min.js', 'https://cdn.pydata.org/bokeh/release/bokeh-compiler-0.12.2.min.js'];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "      Bokeh.$(\"#b4359354-859e-4ac4-b8e9-9405c44e330e\").text(\"BokehJS is loading...\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.2.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.2.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.2.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.2.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((window.Bokeh !== undefined) || (force === \"1\")) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i](window.Bokeh);\n",
       "      }if (force === \"1\") {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < window._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!window._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      window._bokeh_failed_load = true;\n",
       "    } else if (!force) {\n",
       "      var cell = $(\"#b4359354-859e-4ac4-b8e9-9405c44e330e\").parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (window._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(this));"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import csc_matrix\n",
    "from utils import load_json, load_pkl\n",
    "import string\n",
    "import re\n",
    "from operator import itemgetter\n",
    "import nltk\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "from bokeh.plotting import figure, output_notebook,show, ColumnDataSource\n",
    "from bokeh.models.widgets import DataTable, DateFormatter, TableColumn\n",
    "from bokeh.layouts import widgetbox\n",
    "\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "courses = load_json('data/courses.txt')\n",
    "stopwords = load_pkl('data/stopwords.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.1: Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id2name = dict(map(itemgetter('courseId', 'name'),courses))\n",
    "name2id = {v: k for k,v in id2name.items()}\n",
    "np.save('id2name', id2name)\n",
    "np.save('name2id', name2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lmtzr = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def isValidWord(word):\n",
    "    return not word.lower() in stopwords and not word.isdigit() and not word in string.punctuation\n",
    "def processWord(word):\n",
    "    return lmtzr.lemmatize(word.lower())\n",
    "\n",
    "# Returns the bag of words of a text as a dictionary, so the different words as keys and their number of occurence as value\n",
    "def getCourseBagOfWords(text):\n",
    "    # We create a default dict which returns 0 when an item is not in\n",
    "    bow = defaultdict(lambda: 0)\n",
    "    text = nltk.word_tokenize(text)\n",
    "\n",
    "    for idx in range(len(text)):\n",
    "        word = text[idx]\n",
    "        # separate words such that \"MyNameIsChristian\"\n",
    "        # becomes \"My\" \"Name\" \"Is\" \"Christian\"\n",
    "        # however makes sure that word such as \"USA\" and \"imageJ\" are untouched\n",
    "        res = re.findall('[a-zA-Z][^A-Z]*',word)\n",
    "        # We check that we have a match of several words\n",
    "        if len(res) > 1:\n",
    "            # Check if than we dont have a one letter word\n",
    "            if len(min(res,key=len)) > 1:\n",
    "                # delete initial world\n",
    "                del text[idx]\n",
    "                # Add the new words\n",
    "                for offset, match in enumerate(res):\n",
    "                    text.insert(idx+offset,match)\n",
    "                            \n",
    "    for word in text:\n",
    "        if isValidWord(word):\n",
    "            bow[processWord(word)] += 1\n",
    "\n",
    "    return bow\n",
    "\n",
    "# Computes the bag of word for each course\n",
    "# and the global bag of words\n",
    "def getBagOfWords():\n",
    "    globalBagOfWords = defaultdict(lambda: 0)\n",
    "    bagOfWords = {}\n",
    "    for course in courses:\n",
    "        bow = getCourseBagOfWords(course['description'])\n",
    "        bagOfWords[course['courseId']] = bow\n",
    "        for k,v in bow.items():\n",
    "            globalBagOfWords[k] += v\n",
    "    \n",
    "    occurences = sorted(globalBagOfWords.items(), key=itemgetter(1))\n",
    "    # We remove all words with occurences < minBound and > maxBound\n",
    "    # where minBound is the occurence of the lowest 20th term\n",
    "    # and maxBound the occurence of the highest 20th term\n",
    "    minBound = occurences[20][1]\n",
    "    maxBound = occurences[-20][1]\n",
    "    globalBagOfWords = {k: v for k,v in globalBagOfWords.items() if v > minBound and v < maxBound}\n",
    "    for course in bagOfWords.keys():\n",
    "        bagOfWords[course] = {k: v for k,v in bagOfWords[course].items() if k in globalBagOfWords}\n",
    "    return globalBagOfWords, bagOfWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118469\n",
      "7205\n"
     ]
    }
   ],
   "source": [
    "globalBagOfWords, bagOfWords = getBagOfWords()\n",
    "print(sum(globalBagOfWords.values()))\n",
    "print(len(globalBagOfWords.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Explain which ones you implemented and why.\n",
    "We chose to remove all punctuation and all stopwords since there really is no interest in keeping them.\n",
    "\n",
    "We also lemmatize the words using the nltk library, to keep track of similar words and have a more accurate word occurence count.\n",
    "\n",
    "We remove every words that was as present as the 20 most/least present words.\n",
    "\n",
    "## 2.Print the terms in the pre-processed description of the IX class in alphabetical order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words for Internet analytics course:\n",
      "   - acquired\n",
      "   - activity\n",
      "   - ad\n",
      "   - advertisement\n",
      "   - algebra\n",
      "   - algorithm\n",
      "   - analytics\n",
      "   - analyze\n",
      "   - application\n",
      "   - auction\n",
      "   - balance\n",
      "   - based\n",
      "   - cathedra\n",
      "   - chain\n",
      "   - class\n",
      "   - cloud\n",
      "   - clustering\n",
      "   - collection\n",
      "   - com-300\n",
      "   - combination\n",
      "   - communication\n",
      "   - community\n",
      "   - computing\n",
      "   - concrete\n",
      "   - coverage\n",
      "   - current\n",
      "   - data\n",
      "   - datasets\n",
      "   - decade\n",
      "   - dedicated\n",
      "   - designed\n",
      "   - detection\n",
      "   - develop\n",
      "   - dimensionality\n",
      "   - draw\n",
      "   - e-commerce\n",
      "   - effectiveness\n",
      "   - efficiency\n",
      "   - exam\n",
      "   - expected\n",
      "   - explore\n",
      "   - explores\n",
      "   - field\n",
      "   - final\n",
      "   - foundational\n",
      "   - framework\n",
      "   - function\n",
      "   - fundamental\n",
      "   - good\n",
      "   - graph\n",
      "   - hadoop\n",
      "   - hands-on\n",
      "   - homework\n",
      "   - important\n",
      "   - information\n",
      "   - infrastructure\n",
      "   - inspired\n",
      "   - internet\n",
      "   - java\n",
      "   - key\n",
      "   - knowledge\n",
      "   - lab\n",
      "   - laboratory\n",
      "   - large-scale\n",
      "   - linear\n",
      "   - machine\n",
      "   - main\n",
      "   - map-reduce\n",
      "   - markov\n",
      "   - material\n",
      "   - medium\n",
      "   - midterm\n",
      "   - mining\n",
      "   - modeling\n",
      "   - network\n",
      "   - networking\n",
      "   - number\n",
      "   - online\n",
      "   - past\n",
      "   - practical\n",
      "   - practice\n",
      "   - problem\n",
      "   - provide\n",
      "   - question\n",
      "   - real\n",
      "   - real-world\n",
      "   - recommended\n",
      "   - recommender\n",
      "   - reduction\n",
      "   - related\n",
      "   - required\n",
      "   - retrieval\n",
      "   - search\n",
      "   - seek\n",
      "   - self-contained\n",
      "   - service\n",
      "   - session\n",
      "   - social\n",
      "   - spark\n",
      "   - specifically\n",
      "   - start\n",
      "   - statistic\n",
      "   - stochastic\n",
      "   - stream\n",
      "   - structure\n",
      "   - technique\n",
      "   - theoretical\n",
      "   - theory\n",
      "   - topic\n",
      "   - typical\n",
      "   - ubiquitous\n",
      "   - user\n",
      "   - weekly\n",
      "   - work\n",
      "   - world\n"
     ]
    }
   ],
   "source": [
    "ixBow = bagOfWords[name2id['Internet analytics']]\n",
    "print('Words for Internet analytics course:')\n",
    "for words in sorted(ixBow.keys(),key=lambda v: v.upper()):\n",
    "    print('   -',words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.2: Term-document matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analytics \t 2.01373274539\n",
      "acquired \t 0.863028319455\n",
      "develop \t 0.863028319455\n",
      "main \t 0.839055310581\n",
      "application \t 0.805493098158\n",
      "graph \t 0.671244248465\n",
      "linear \t 0.671244248465\n",
      "algebra \t 0.671244248465\n",
      "field \t 0.671244248465\n",
      "inspired \t 0.671244248465\n",
      "specifically \t 0.671244248465\n",
      "seek \t 0.604119823618\n",
      "statistic \t 0.604119823618\n",
      "lab \t 0.549199839653\n",
      "work \t 0.503433186349\n"
     ]
    }
   ],
   "source": [
    "globalBagOfWords, bagOfWords = getBagOfWords()\n",
    "\n",
    "# Som helpers variable\n",
    "numTerms = len(globalBagOfWords.keys())\n",
    "numCourses = len(bagOfWords.keys())\n",
    "termsOrder = list(enumerate(globalBagOfWords.keys()))\n",
    "coursesOrder = list(enumerate(bagOfWords.keys()))\n",
    "idx2Term = {i[0]: i[1] for i in termsOrder}\n",
    "term2Idx = {v: k for k,v in idx2Term.items()}\n",
    "idx2Course = {i[0]: i[1] for i in coursesOrder}\n",
    "course2Idx = {v: k for k,v in idx2Course.items()}\n",
    "\n",
    "np.save('idx2Term', idx2Term)\n",
    "np.save('term2Idx', term2Idx)\n",
    "np.save('idx2Course', idx2Course)\n",
    "np.save('course2Idx', course2Idx)\n",
    "\n",
    "overallFreq = np.zeros(numTerms)\n",
    "row = []\n",
    "col = []\n",
    "data  = []\n",
    "\n",
    "# We construct the term document matrix\n",
    "for courseIdx, course in coursesOrder:\n",
    "    if(len(bagOfWords[course]) == 0):\n",
    "        continue\n",
    "    docMax = max(bagOfWords[course].values())\n",
    "    for termIdx, term in termsOrder:\n",
    "        if(term not in bagOfWords[course]):\n",
    "            continue\n",
    "        \n",
    "        row.append(termIdx)\n",
    "        col.append(courseIdx)\n",
    "        # We use the formual seen in class\n",
    "        data.append(bagOfWords[course][term]/docMax)\n",
    "        # We construct at the same time what we need for the idf\n",
    "        overallFreq[termIdx] += 1\n",
    "\n",
    "# We construct the sparse matrix\n",
    "tf_idf = csr_matrix((data,(row,col)))\n",
    "\n",
    "# We compute the idf\n",
    "overallFreq = np.log(numCourses/overallFreq)\n",
    "\n",
    "np.save('idf',overallFreq)\n",
    "\n",
    "# tf_idf = tf.copy()\n",
    "# We multiply each row by the corresponding idf\n",
    "tf_idf.data *= overallFreq.repeat(np.diff(tf_idf.indptr))\n",
    "\n",
    "np.save('X',tf_idf)\n",
    "\n",
    "# We convert the Compressed Sparse Row matrix to Compressed Sparse Column matrix\n",
    "# Since we need a particular course\n",
    "csc = tf_idf.tocsc()\n",
    "# Get the index of the course\n",
    "idx = course2Idx[name2id['Internet analytics']]\n",
    "\n",
    "# Get the offsets\n",
    "offset = csc.indptr[idx]\n",
    "offsetEnd = csc.indptr[idx+1]\n",
    "\n",
    "# We find the top 15 values' indices in the data and find the term corresponding to those\n",
    "topTerms = [(idx2Term[csc.indices[offset+index]],tf_idf.data[offset:offsetEnd][index])  for index in np.argsort(-tf_idf.data[offset:offsetEnd])[:15]]\n",
    "for term, score in topTerms:\n",
    "    print(term,'\\t',score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.3: Document similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docSim(di,dj):\n",
    "    # Norm is done using underlying data vector, much faster than using transpose\n",
    "    return (di.dot(dj)/(np.sqrt((di.data ** 2).sum())*np.sqrt((dj.data ** 2).sum()))).A[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top courses with query \"markov chain\":\n",
      "   - Applied probability & stochastic processes \t0.564\n",
      "   - Applied stochastic processes \t0.563\n",
      "   - Markov chains and algorithmic applications \t0.357\n",
      "   - Supply chain management \t0.355\n",
      "   - Statistical Sequence Processing \t0.299\n",
      "Top courses with query \"facebook\":\n",
      "   - Computational Social Media \t0.186\n"
     ]
    }
   ],
   "source": [
    "# Gives the top 5 course for given terms\n",
    "# Takes a space separated list of terms as argument\n",
    "def query(terms):\n",
    "    terms = terms.split(' ')\n",
    "    \n",
    "    # We get the tf-idf for the terms, tf is always 1 and idf was already computed\n",
    "    data = [overallFreq[term2Idx[term]] for term in terms]\n",
    "    cols = [term2Idx[term] for term in terms]\n",
    "    rows = [0] * len(terms)\n",
    "    \n",
    "    # We create the query of shape 1 x M\n",
    "    query = csc_matrix((data,(rows,cols)),shape=(1,numTerms))\n",
    "    \n",
    "    # We run document similarity with the query for all the documents\n",
    "    results = np.array([docSim(query,tf_idf.getcol(doc)) for doc in range(numCourses)])\n",
    "    top = np.argsort(-results)[:5]\n",
    "    # We filter all courses with null scores\n",
    "    top = [course for course in top if results[course] != 0.0]\n",
    "    \n",
    "    print('Top courses with query \"'+\" \".join(terms)+'\":')\n",
    "    for course in top:\n",
    "        print('   -',id2name[idx2Course[course]],'\\t%.3f'%results[course])\n",
    "query('markov chain')\n",
    "query('facebook')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
