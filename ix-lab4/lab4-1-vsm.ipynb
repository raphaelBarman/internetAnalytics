{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text 1: Vector space models\n",
    "**Internet Analytics - Lab 4**\n",
    "\n",
    "---\n",
    "\n",
    "**Group:** R\n",
    "**Names:**\n",
    "\n",
    "* Raphael Strebel\n",
    "* Raphaël Barman\n",
    "* Thierry Bossy\n",
    "\n",
    "---\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "*This is a template for part 1 of the lab. Clearly write your answers, comments and interpretations in Markodown cells. Don't forget that you can add $\\LaTeX$ equations in these cells. Feel free to add or remove any cell.*\n",
    "\n",
    "*Please properly comment your code. Code readability will be considered for grading. To avoid long cells of codes in the notebook, you can also embed long python functions and classes in a separate module. Don’t forget to hand in your module if that is the case. In multiple exercises, you are required to come up with your own method to solve various problems. Be creative and clearly motivate and explain your methods. Creativity and clarity will be considered for grading.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"http://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"d0ea9ddc-844b-4515-8199-3b4f671121ee\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(global) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = \"1\";\n",
       "\n",
       "  if (typeof (window._bokeh_onload_callbacks) === \"undefined\" || force !== \"\") {\n",
       "    window._bokeh_onload_callbacks = [];\n",
       "    window._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "\n",
       "  \n",
       "  if (typeof (window._bokeh_timeout) === \"undefined\" || force !== \"\") {\n",
       "    window._bokeh_timeout = Date.now() + 5000;\n",
       "    window._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    if (window.Bokeh !== undefined) {\n",
       "      Bokeh.$(\"#d0ea9ddc-844b-4515-8199-3b4f671121ee\").text(\"BokehJS successfully loaded.\");\n",
       "    } else if (Date.now() < window._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    window._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    delete window._bokeh_onload_callbacks\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    window._bokeh_onload_callbacks.push(callback);\n",
       "    if (window._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    window._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        window._bokeh_is_loading--;\n",
       "        if (window._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"d0ea9ddc-844b-4515-8199-3b4f671121ee\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid 'd0ea9ddc-844b-4515-8199-3b4f671121ee' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = ['https://cdn.pydata.org/bokeh/release/bokeh-0.12.2.min.js', 'https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.2.min.js', 'https://cdn.pydata.org/bokeh/release/bokeh-compiler-0.12.2.min.js'];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "      Bokeh.$(\"#d0ea9ddc-844b-4515-8199-3b4f671121ee\").text(\"BokehJS is loading...\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.2.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.2.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.2.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.2.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((window.Bokeh !== undefined) || (force === \"1\")) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i](window.Bokeh);\n",
       "      }if (force === \"1\") {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < window._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!window._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      window._bokeh_failed_load = true;\n",
       "    } else if (!force) {\n",
       "      var cell = $(\"#d0ea9ddc-844b-4515-8199-3b4f671121ee\").parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (window._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(this));"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from utils import load_json, load_pkl\n",
    "import string\n",
    "import re\n",
    "from operator import itemgetter\n",
    "import nltk\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "from bokeh.plotting import figure, output_notebook,show, ColumnDataSource\n",
    "from bokeh.models.widgets import DataTable, DateFormatter, TableColumn\n",
    "from bokeh.layouts import widgetbox\n",
    "\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "courses = load_json('data/courses.txt')\n",
    "stopwords = load_pkl('data/stopwords.pkl')\n",
    "stopwords.add('-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.1: Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id2name = dict(map(itemgetter('courseId', 'name'),courses))\n",
    "name2id = {v: k for k,v in id2name.items()}\n",
    "np.save('id2name', id2name)\n",
    "np.save('name2id', name2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### WITH BIGRAM\n",
    "\n",
    "lmtzr = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def isValidWord(word):\n",
    "    return not word in stopwords and not word.isdigit() and not word in string.punctuation\n",
    "def isValidBigram(bigram):\n",
    "    return isValidWord(bigram[0]) and isValidWord(bigram[1])\n",
    "def processBigram(bigram):\n",
    "    return processWord(bigram[0]) + \" \" + processWord(bigram[1])\n",
    "def processWord(word):\n",
    "    return lmtzr.lemmatize(word.lower())\n",
    "\n",
    "# Returns the bag of words of a text as a dictionary, so the different words as keys and their number of occurence as value\n",
    "def getCourseBagOfWords(text):\n",
    "    # We create a default dict which returns 0 when an item is not in\n",
    "    bow = defaultdict(lambda: 0)\n",
    "    text = nltk.word_tokenize(text)\n",
    "\n",
    "    for idx in range(len(text)):\n",
    "        word = text[idx]\n",
    "        # separate words such that \"MyNameIsChristian\"\n",
    "        # becomes \"My\" \"Name\" \"Is\" \"Christian\"\n",
    "        # however makes sure that word such as \"USA\" and \"imageJ\" are untouched\n",
    "        res = re.findall('[a-zA-Z][^A-Z]*',word)\n",
    "        # We check that we have a match of several words\n",
    "        if len(res) > 1:\n",
    "            # Check if than we dont have a one letter word\n",
    "            if len(min(res,key=len)) > 1:\n",
    "                # delete initial world\n",
    "                del text[idx]\n",
    "                # Add the new words\n",
    "                for offset, match in enumerate(res):\n",
    "                    text.insert(idx+offset,match)\n",
    "                    \n",
    "    text = [processWord(word) for word in text if isValidWord(word)]\n",
    "    text.extend([processBigram(bigram) for bigram in ngrams(text,2) if isValidBigram(bigram)])\n",
    "    for word in text:\n",
    "        bow[word] += 1\n",
    "\n",
    "    return bow\n",
    "\n",
    "# Computes the bag of word for each course\n",
    "# and the global bag of words\n",
    "def getBagOfWords():\n",
    "    globalBagOfWords = defaultdict(lambda: 0)\n",
    "    bagOfWords = {}\n",
    "    for course in courses:\n",
    "        bow = getCourseBagOfWords(course['description'])\n",
    "        bagOfWords[course['courseId']] = bow\n",
    "        for k,v in bow.items():\n",
    "            globalBagOfWords[k] += v\n",
    "    \n",
    "    occurences = sorted(globalBagOfWords.items(), key=itemgetter(1))\n",
    "    print(occurences[-9])\n",
    "    # We remove all words with occurences < minBound and > maxBound\n",
    "    # where minBound is the occurence of the lowest 20th term\n",
    "    # and maxBound the occurence of the highest 20th term\n",
    "    minBound = occurences[20][1]\n",
    "    maxBound = occurences[-20][1]\n",
    "    globalBagOfWords = {k: v for k,v in globalBagOfWords.items() if v > minBound and v < maxBound}\n",
    "    for course in bagOfWords.keys():\n",
    "        bagOfWords[course] = {k: v for k,v in bagOfWords[course].items() if k in globalBagOfWords}\n",
    "    return globalBagOfWords, bagOfWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmtzr = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def isValidWord(word):\n",
    "    return not word in stopwords and not word.isdigit() and not word in string.punctuation\n",
    "def isValidBigram(bigram):\n",
    "    return isValidWord(bigram[0]) and isValidWord(bigram[1])\n",
    "def processBigram(bigram):\n",
    "    return processWord(bigram[0]) + \" \" + processWord(bigram[1])\n",
    "def processWord(word):\n",
    "    return lmtzr.lemmatize(word.lower())\n",
    "\n",
    "# Returns the bag of words of a text as a dictionary, so the different words as keys and their number of occurence as value\n",
    "def getCourseBagOfWords(text):\n",
    "    # We create a default dict which returns 0 when an item is not in\n",
    "    bow = defaultdict(lambda: 0)\n",
    "    text = nltk.word_tokenize(text)\n",
    "\n",
    "    for idx in range(len(text)):\n",
    "        word = text[idx]\n",
    "        # separate words such that \"MyNameIsChristian\"\n",
    "        # becomes \"My\" \"Name\" \"Is\" \"Christian\"\n",
    "        # however makes sure that word such as \"USA\" and \"imageJ\" are untouched\n",
    "        res = re.findall('[a-zA-Z][^A-Z]*',word)\n",
    "        # We check that we have a match of several words\n",
    "        if len(res) > 1:\n",
    "            # Check if than we dont have a one letter word\n",
    "            if len(min(res,key=len)) > 1:\n",
    "                # delete initial world\n",
    "                del text[idx]\n",
    "                # Add the new words\n",
    "                for offset, match in enumerate(res):\n",
    "                    text.insert(idx+offset,match)\n",
    "                    \n",
    "    text = [processWord(word) for word in text if isValidWord(word)]\n",
    "        \n",
    "    for word in text:\n",
    "        bow[word] += 1\n",
    "\n",
    "    return bow\n",
    "\n",
    "# Computes the bag of word for each course\n",
    "# and the global bag of words\n",
    "def getBagOfWords():\n",
    "    globalBagOfWords = defaultdict(lambda: 0)\n",
    "    bagOfWords = {}\n",
    "    for course in courses:\n",
    "        bow = getCourseBagOfWords(course['description'])\n",
    "        bagOfWords[course['courseId']] = bow\n",
    "        for k,v in bow.items():\n",
    "            globalBagOfWords[k] += v\n",
    "    \n",
    "    occurences = sorted(globalBagOfWords.items(), key=itemgetter(1))\n",
    "    # We remove all words with occurences < minBound and > maxBound\n",
    "    # where minBound is the occurence of the lowest 20th term\n",
    "    # and maxBound the occurence of the highest 20th term\n",
    "    minBound = occurences[20][1]\n",
    "    maxBound = occurences[-20][1]\n",
    "    globalBagOfWords = {k: v for k,v in globalBagOfWords.items() if v > minBound and v < maxBound}\n",
    "    for course in bagOfWords.keys():\n",
    "        bagOfWords[course] = {k: v for k,v in bagOfWords[course].items() if k in globalBagOfWords}\n",
    "    return globalBagOfWords, bagOfWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('course', 807)\n",
      "123750\n",
      "7358\n"
     ]
    }
   ],
   "source": [
    "globalBagOfWords, bagOfWords = getBagOfWords()\n",
    "print(sum(globalBagOfWords.values()))\n",
    "print(len(globalBagOfWords.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Explain which ones you implemented and why.\n",
    "We chose to remove all punctuation and all stopwords since there really is no interest in keeping them.\n",
    "\n",
    "We also lemmatize the words using the nltk library, to keep track of similar words and have a more accurate word occurence count.\n",
    "\n",
    "## 2.Print the terms in the pre-processed description of the IX class in alphabetical order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words for Internet analytics course:\n",
      "   - acquired\n",
      "   - activity\n",
      "   - ad\n",
      "   - advertisement\n",
      "   - algebra\n",
      "   - algorithm\n",
      "   - analytics\n",
      "   - analyze\n",
      "   - application\n",
      "   - auction\n",
      "   - balance\n",
      "   - based\n",
      "   - cathedra\n",
      "   - chain\n",
      "   - class\n",
      "   - cloud\n",
      "   - clustering\n",
      "   - collection\n",
      "   - com-300\n",
      "   - combination\n",
      "   - communication\n",
      "   - community\n",
      "   - computing\n",
      "   - concrete\n",
      "   - coverage\n",
      "   - current\n",
      "   - data\n",
      "   - datasets\n",
      "   - decade\n",
      "   - dedicated\n",
      "   - designed\n",
      "   - detection\n",
      "   - develop\n",
      "   - dimensionality\n",
      "   - draw\n",
      "   - e-commerce\n",
      "   - effectiveness\n",
      "   - efficiency\n",
      "   - ex\n",
      "   - exam\n",
      "   - expected\n",
      "   - explore\n",
      "   - explores\n",
      "   - field\n",
      "   - final\n",
      "   - for\n",
      "   - foundational\n",
      "   - framework\n",
      "   - function\n",
      "   - fundamental\n",
      "   - good\n",
      "   - graph\n",
      "   - hadoop\n",
      "   - hands-on\n",
      "   - homework\n",
      "   - important\n",
      "   - information\n",
      "   - infrastructure\n",
      "   - inspired\n",
      "   - internet\n",
      "   - java\n",
      "   - key\n",
      "   - keywords\n",
      "   - knowledge\n",
      "   - lab\n",
      "   - laboratory\n",
      "   - large-scale\n",
      "   - linear\n",
      "   - machine\n",
      "   - main\n",
      "   - map-reduce\n",
      "   - markov\n",
      "   - material\n",
      "   - medium\n",
      "   - midterm\n",
      "   - mining\n",
      "   - modeling\n",
      "   - network\n",
      "   - networking\n",
      "   - number\n",
      "   - online\n",
      "   - past\n",
      "   - practical\n",
      "   - practice\n",
      "   - problem\n",
      "   - provide\n",
      "   - question\n",
      "   - real\n",
      "   - real-world\n",
      "   - recommended\n",
      "   - recommender\n",
      "   - reduction\n",
      "   - related\n",
      "   - required\n",
      "   - retrieval\n",
      "   - search\n",
      "   - seek\n",
      "   - self-contained\n",
      "   - service\n",
      "   - session\n",
      "   - social\n",
      "   - spark\n",
      "   - specifically\n",
      "   - start\n",
      "   - statistic\n",
      "   - stochastic\n",
      "   - stream\n",
      "   - structure\n",
      "   - teaching\n",
      "   - technique\n",
      "   - theoretical\n",
      "   - theory\n",
      "   - this\n",
      "   - together\n",
      "   - topic\n",
      "   - typical\n",
      "   - ubiquitous\n",
      "   - user\n",
      "   - weekly\n",
      "   - work\n",
      "   - world\n"
     ]
    }
   ],
   "source": [
    "ixBow = bagOfWords[name2id['Internet analytics']]\n",
    "print('Words for Internet analytics course:')\n",
    "for words in sorted(ixBow.keys(),key=lambda v: v.upper()):\n",
    "    print('   -',words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.2: Term-document matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('course', 807)\n"
     ]
    }
   ],
   "source": [
    "globalBagOfWords, bagOfWords = getBagOfWords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numTerms = len(globalBagOfWords.keys())\n",
    "numCourses = len(bagOfWords.keys())\n",
    "termsOrder = list(enumerate(globalBagOfWords.keys()))\n",
    "coursesOrder = list(enumerate(bagOfWords.keys()))\n",
    "idx2Term = {i[0]: i[1] for i in termsOrder}\n",
    "term2Idx = {v: k for k,v in idx2Term.items()}\n",
    "idx2Course = {i[0]: i[1] for i in coursesOrder}\n",
    "course2Idx = {v: k for k,v in idx2Course.items()}\n",
    "\n",
    "np.save('idx2Term', idx2Term)\n",
    "np.save('term2Idx', term2Idx)\n",
    "np.save('idx2Course', idx2Course)\n",
    "np.save('course2Idx', course2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "overallFreq = np.zeros(numTerms)\n",
    "row = []\n",
    "col = []\n",
    "data  = []\n",
    "\n",
    "# We construct the term document matrix\n",
    "for courseIdx, course in coursesOrder:\n",
    "    if(len(bagOfWords[course]) == 0):\n",
    "        continue\n",
    "    docMax = max(bagOfWords[course].values())\n",
    "    for termIdx, term in termsOrder:\n",
    "        if(term not in bagOfWords[course]):\n",
    "            continue\n",
    "        col.append(courseIdx)\n",
    "        row.append(termIdx)\n",
    "        # We use the formual seen in class\n",
    "        data.append(bagOfWords[course][term]/docMax)\n",
    "        # We construct at the same time what we need for the idf\n",
    "        overallFreq[termIdx] += 1\n",
    "\n",
    "# We construct the sparse matrix\n",
    "tf = csr_matrix((data,(row,col)))\n",
    "# We compute the idf\n",
    "overallFreq = np.log(numCourses/overallFreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good \t 2.83790818836\n",
      "current \t 2.83790818836\n",
      "keywords \t 2.43249273288\n",
      "topic \t 2.12843114127\n",
      "main \t 1.70274491302\n",
      "required \t 1.41895409418\n",
      "based \t 1.05438940548\n",
      "laboratory \t 0.948950464934\n",
      "this \t 0.81083091096\n",
      "question \t 0.70947704709\n",
      "session \t 0.70947704709\n",
      "ex \t 0.70947704709\n",
      "hands-on \t 0.63263364329\n",
      "networking \t 0.63263364329\n",
      "develop \t 0.63263364329\n"
     ]
    }
   ],
   "source": [
    "tf_idf = tf.copy()\n",
    "# We multiply each row by the corresponding idf\n",
    "tf_idf.data *= overallFreq.repeat(np.diff(tf_idf.indptr))\n",
    "\n",
    "np.save('X',tf_idf)\n",
    "# We convert the Compressed Sparse Row matrix to Compressed Sparse Column matrix\n",
    "# Since we need a particular course\n",
    "csc = tf_idf.tocsc()\n",
    "# Get the index of the course\n",
    "idx = course2Idx[name2id['Internet analytics']]\n",
    "# Get the offsets\n",
    "offset = csc.indptr[idx]\n",
    "offsetEnd = csc.indptr[idx+1]\n",
    "\n",
    "# We find the top 15 values' indices in the data and find the term corresponding to those\n",
    "topTerms = [(idx2Term[csc.indices[offset+index]],tf_idf.data[offset:offsetEnd][index])  for index in np.argsort(-tf_idf.data[offset:offsetEnd])[:15]]\n",
    "for term, score in topTerms:\n",
    "    print(term,'\\t',score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.3: Document similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def docSimilarity(di,dj):\n",
    "    return di.T.dot(dj)/(np.sqrt(di.T.dot(di))*np.sqrt(dj.T.dot(dj)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########### WITH BIGRAM\n",
    "# Gives the top 5 course for given terms\n",
    "# Takes a space separated list of terms as argument\n",
    "def query(terms):\n",
    "    # Initialize with the first term\n",
    "    row = tf_idf.getrow(term2Idx[terms])\n",
    "    # top in the \"local\" (csr) row\n",
    "    topLocal = np.argsort(-row.data)[:5]\n",
    "    #top in term of courses index\n",
    "    top = row.indices[topLocal]\n",
    "    topCourses = list(map(lambda x: id2name[idx2Course[x]],top))\n",
    "    print('Top courses with query \"'+\" \".join(terms)+'\":')\n",
    "    for idx,course in zip(topLocal,topCourses):\n",
    "        print('   -',course,'\\t%.3f'%row.data[idx])\n",
    "    numRes = len(top)\n",
    "    cmp = np.zeros((numRes,numRes))\n",
    "    for i in range(numRes):\n",
    "        for j in range(numRes):\n",
    "            cmp[i][j] = docSimilarity(tf_idf.getcol(top[i]),tf_idf.getcol(top[j]))\n",
    "    print('Comparison matrix:')\n",
    "    print(cmp)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gives the top 5 course for given terms\n",
    "# Takes a space separated list of terms as argument\n",
    "def query(terms):\n",
    "    terms = terms.split(' ')\n",
    "    # Initialize with the first term\n",
    "    row = tf_idf.getrow(term2Idx[lmtzr.lemmatize(terms[0])])\n",
    "    # If there is more, add them\n",
    "    for term in terms[1:]:\n",
    "        row += tf_idf.getrow(term2Idx[lmtzr.lemmatize(term)])\n",
    "    # top in the \"local\" (csr) row\n",
    "    topLocal = np.argsort(-row.data)[:5]\n",
    "    #top in term of courses index\n",
    "    top = row.indices[topLocal]\n",
    "    topCourses = list(map(lambda x: id2name[idx2Course[x]],top))\n",
    "    print('Top courses with query \"'+\" \".join(terms)+'\":')\n",
    "    for idx,course in zip(topLocal,topCourses):\n",
    "        print('   -',course,'\\t%.3f'%row.data[idx])\n",
    "    numRes = len(top)\n",
    "    cmp = np.zeros((numRes,numRes))\n",
    "    for i in range(numRes):\n",
    "        for j in range(numRes):\n",
    "            cmp[i][j] = docSimilarity(tf_idf.getcol(top[i]),tf_idf.getcol(top[j]))\n",
    "    print('Comparison matrix:')\n",
    "    print(cmp)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top courses with query \"markov chain\":\n",
      "   - Applied probability & stochastic processes \t5.514\n",
      "   - Markov chains and algorithmic applications \t4.815\n",
      "   - Statistical Sequence Processing \t3.805\n",
      "   - Mathematical models in supply chain management \t3.418\n",
      "   - Supply chain management \t3.174\n",
      "Comparison matrix:\n",
      "[[ 1.          0.2970632   0.24668716  0.11859983  0.1758871 ]\n",
      " [ 0.2970632   1.          0.13099066  0.12211414  0.13941292]\n",
      " [ 0.24668716  0.13099066  1.          0.01665724  0.00379282]\n",
      " [ 0.11859983  0.12211414  0.01665724  1.          0.57552457]\n",
      " [ 0.1758871   0.13941292  0.00379282  0.57552457  1.        ]]\n",
      "\n",
      "\n",
      "Top courses with query \"facebook\":\n",
      "   - Computational Social Media \t1.066\n",
      "Comparison matrix:\n",
      "[[ 1.]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query('markov chain')\n",
    "query('facebook')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
