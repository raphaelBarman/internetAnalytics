{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text 1: Vector space models\n",
    "**Internet Analytics - Lab 4**\n",
    "\n",
    "---\n",
    "\n",
    "**Group:** R\n",
    "**Names:**\n",
    "\n",
    "* Raphael Strebel\n",
    "* Raphaël Barman\n",
    "* Thierry Bossy\n",
    "\n",
    "---\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "*This is a template for part 1 of the lab. Clearly write your answers, comments and interpretations in Markodown cells. Don't forget that you can add $\\LaTeX$ equations in these cells. Feel free to add or remove any cell.*\n",
    "\n",
    "*Please properly comment your code. Code readability will be considered for grading. To avoid long cells of codes in the notebook, you can also embed long python functions and classes in a separate module. Don’t forget to hand in your module if that is the case. In multiple exercises, you are required to come up with your own method to solve various problems. Be creative and clearly motivate and explain your methods. Creativity and clarity will be considered for grading.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"http://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"9f9c43ed-cfd4-4aa1-89b1-d4c072ae11ef\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(global) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = \"1\";\n",
       "\n",
       "  if (typeof (window._bokeh_onload_callbacks) === \"undefined\" || force !== \"\") {\n",
       "    window._bokeh_onload_callbacks = [];\n",
       "    window._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "\n",
       "  \n",
       "  if (typeof (window._bokeh_timeout) === \"undefined\" || force !== \"\") {\n",
       "    window._bokeh_timeout = Date.now() + 5000;\n",
       "    window._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    if (window.Bokeh !== undefined) {\n",
       "      Bokeh.$(\"#9f9c43ed-cfd4-4aa1-89b1-d4c072ae11ef\").text(\"BokehJS successfully loaded.\");\n",
       "    } else if (Date.now() < window._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    window._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    delete window._bokeh_onload_callbacks\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    window._bokeh_onload_callbacks.push(callback);\n",
       "    if (window._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    window._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        window._bokeh_is_loading--;\n",
       "        if (window._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"9f9c43ed-cfd4-4aa1-89b1-d4c072ae11ef\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '9f9c43ed-cfd4-4aa1-89b1-d4c072ae11ef' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = ['https://cdn.pydata.org/bokeh/release/bokeh-0.12.2.min.js', 'https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.2.min.js', 'https://cdn.pydata.org/bokeh/release/bokeh-compiler-0.12.2.min.js'];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "      Bokeh.$(\"#9f9c43ed-cfd4-4aa1-89b1-d4c072ae11ef\").text(\"BokehJS is loading...\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.2.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.2.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.2.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.2.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((window.Bokeh !== undefined) || (force === \"1\")) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i](window.Bokeh);\n",
       "      }if (force === \"1\") {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < window._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!window._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      window._bokeh_failed_load = true;\n",
       "    } else if (!force) {\n",
       "      var cell = $(\"#9f9c43ed-cfd4-4aa1-89b1-d4c072ae11ef\").parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (window._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(this));"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from utils import load_json, load_pkl\n",
    "import string\n",
    "import re\n",
    "from operator import itemgetter\n",
    "import nltk\n",
    "import math\n",
    "\n",
    "from bokeh.plotting import figure, output_notebook,show, ColumnDataSource\n",
    "from bokeh.models.widgets import DataTable, DateFormatter, TableColumn\n",
    "from bokeh.layouts import widgetbox\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".bk-root .bk-slick-header-column.bk-ui-state-default {\n",
    "height: 25px!important;\n",
    "}\n",
    "</style>\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "courses = load_json('data/courses.txt')\n",
    "stopwords = load_pkl('data/stopwords.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.1: Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id2name = dict(map(itemgetter('courseId', 'name'),courses))\n",
    "name2id = {v: k for k,v in id2name.items()}\n",
    "np.save('id2name', id2name)\n",
    "np.save('name2id', name2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lmtzr = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Add a word to the bag of word given as parameter\n",
    "def add2bow(word, bow):\n",
    "    #newbow = bow.copy()\n",
    "    if word not in bow:\n",
    "        bow[word] = 0\n",
    "    bow[word] += 1\n",
    "    return bow\n",
    "\n",
    "# Merges to bag of words\n",
    "def mergeBow(bow1, bow2):\n",
    "    #newbow = bow1.copy()\n",
    "    for word, occ in bow2.items():\n",
    "        if word not in bow1:\n",
    "            bow1[word] = 0\n",
    "        bow1[word] += occ\n",
    "    return bow1\n",
    "\n",
    "# Returns the bag of words of a text as a dictionary, so the different words as keys and their number of occurence as value\n",
    "def getBagOfWords(text):\n",
    "    bow = {}\n",
    "    text = text.replace('\\xa0', ' ')\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('','',string.punctuation))\n",
    "    text = text.split(' ')\n",
    "    for idx in range(len(text)):\n",
    "        word = text[idx]\n",
    "        # separate words such that \"MyNameIsChristian\" becomes \"My\" \"Name\" \"Is\" \"Christian\"\n",
    "        res = re.findall('[a-zA-Z][^A-Z]*',word)\n",
    "        if res:\n",
    "            if len(min(res,key=len)) != 1:\n",
    "                if len(res) > 0:\n",
    "                    text[idx] = ''\n",
    "                for match in res:\n",
    "                    text.append(match)\n",
    "    text = [x for x in text if x != '']\n",
    "    for idx in range(len(text)):\n",
    "        word = text[idx]\n",
    "        # Keep words that are only upper case as such (we don't want IT to become it) and put all others as lower case\n",
    "        if not word.isupper():\n",
    "            word = word.lower()\n",
    "        # Lemmatize all non-digit words \n",
    "        if not word.isdigit() and not word in stopwords:\n",
    "            bow = add2bow(lmtzr.lemmatize(word),bow)\n",
    "            #add2bow(stemmer.stem(word),bow)\n",
    "#     for word in textCopy:\n",
    "#         if not word.isupper():\n",
    "#             word = word.lower()\n",
    "#         if not word.isdigit():\n",
    "#             text.add(lmtzr.lemmatize(word))\n",
    "#     for word in stopwords:\n",
    "#         try:\n",
    "#             text.remove(word)\n",
    "#         except KeyError:\n",
    "#             continue\n",
    "    return bow\n",
    "\n",
    "\n",
    "# Compute bag of words for the description of every course, then merge them and return the global bag of words\n",
    "def getGlobalBagOfWords():\n",
    "    globalBagOfWords = {}\n",
    "    bagOfWords = {}\n",
    "    for course in courses:\n",
    "        #localBow = {}\n",
    "        localBow = getBagOfWords(course['description'])\n",
    "        bagOfWords[course['courseId']] = localBow\n",
    "        localBow = mergeBow(globalBagOfWords,localBow)\n",
    "    occurences = sorted(globalBagOfWords.items(), key=itemgetter(1))\n",
    "    # We remove all words with occurences < minBound and > maxBound\n",
    "    minBound = occurences[9][1]\n",
    "    maxBound = occurences[-9][1]\n",
    "    globalBagOfWords = {k: v for k,v in globalBagOfWords.items() if v > minBound and v < maxBound}\n",
    "    for course in bagOfWords.keys():\n",
    "        bagOfWords[course] = {k: v for k,v in bagOfWords[course].items() if k in globalBagOfWords}\n",
    "    return globalBagOfWords, bagOfWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375351\n",
      "6947\n"
     ]
    }
   ],
   "source": [
    "globalBagOfWords = {}\n",
    "globalBagOfWords, bagOfWords = getGlobalBagOfWords()\n",
    "print(sum(globalBagOfWords.values()))\n",
    "print(len(globalBagOfWords.keys()))\n",
    "#getBagOfWords(courses[1]['description'])\n",
    "#for course in courses:\n",
    "#    bow = getBagOfWords(course['description'])\n",
    "#    bagOfWords[course['courseId']] = bow\n",
    "#    mergeBow(globalBagOfWord,bow)\n",
    "#test_course = course.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('COM300', 1), ('acquired', 1), ('activity', 1), ('ad', 2), ('advertisement', 1), ('algebra', 2), ('algorithm', 2), ('analytics', 2), ('analyze', 1), ('application', 2), ('assessment', 1), ('auction', 2), ('balance', 1), ('based', 2), ('basic', 3), ('cathedra', 1), ('chain', 1), ('class', 3), ('cloud', 1), ('clustering', 2), ('collection', 1), ('combination', 1), ('communication', 1), ('community', 2), ('computing', 2), ('concept', 2), ('concrete', 1), ('coverage', 1), ('current', 1), ('data', 6), ('datasets', 2), ('decade', 1), ('dedicated', 1), ('designed', 1), ('detection', 2), ('develop', 1), ('dimensionality', 1), ('draw', 1), ('ecommerce', 2), ('effectiveness', 1), ('efficiency', 1), ('end', 1), ('exam', 1), ('expected', 1), ('explore', 4), ('explores', 1), ('field', 1), ('final', 1), ('foundational', 1), ('framework', 1), ('function', 1), ('fundamental', 1), ('good', 1), ('graph', 2), ('hadoop', 2), ('handson', 1), ('homework', 2), ('important', 1), ('information', 2), ('infrastructure', 1), ('inspired', 1), ('internet', 2), ('java', 1), ('key', 1), ('keywords', 1), ('knowledge', 1), ('lab', 3), ('laboratory', 1), ('largescale', 3), ('lecture', 2), ('linear', 2), ('machine', 2), ('main', 1), ('mapreduce', 1), ('markov', 1), ('material', 2), ('medium', 1), ('midterm', 1), ('mining', 3), ('modeling', 1), ('network', 1), ('networking', 3), ('number', 2), ('online', 5), ('outcome', 1), ('past', 1), ('practical', 1), ('practice', 1), ('prerequisite', 1), ('problem', 2), ('project', 1), ('provide', 1), ('question', 1), ('real', 1), ('realworld', 4), ('recommended', 1), ('recommender', 2), ('reduction', 1), ('related', 1), ('required', 1), ('retrieval', 1), ('search', 1), ('seek', 1), ('selfcontained', 1), ('service', 5), ('session', 2), ('social', 5), ('spark', 1), ('specifically', 1), ('start', 1), ('statistic', 1), ('stochastic', 1), ('stream', 2), ('structure', 1), ('teaching', 1), ('technique', 1), ('theoretical', 1), ('theory', 1), ('topic', 1), ('typical', 1), ('ubiquitous', 1), ('user', 1), ('weekly', 1), ('work', 1), ('world', 1)]\n"
     ]
    }
   ],
   "source": [
    "sortedWords = sorted(bagOfWords[name2id['Internet analytics']].items(), key=itemgetter(0))\n",
    "print(sortedWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose to remove all punctuation and all stopwords since there really is no interest in keeping them.\n",
    "We also lemmatize the words using the nltk library, to keep track of similar words and have a more accurate word occurence count. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Image Processing for Life Science', 'courseId': 'BIO-695', 'description': 'This course intends to teach image processing with a strong emphasis of applications in life sciences. The idea is to enable the participants to solve image processing questions via workflows independently. Content Over the last decades, the images arising from microscopes in Life Sciences went from being a qualitative support of scientific evidence to a quantitative resource. To obtain good quality data from digital images, be it from a photograph of a Western blot, a TEM slice or a multi-channel confocal time-lapse stack, scientists must understand the underlying processes leading to the extracted information. Of similar importance is the software used to obtain the data. This course makes use of the ImageJ (FIJI package) as well as other open-source tools to ensure maximum reproducibility and protocol transfer of the analysis pipelines. The course will span 14 weeks with 1h30 of lecture per week, as well as exercises to complete outside of the course and will enable to students to establish image analysis workflows autonomously. Note This course is open to max. 16 students selected by the organizer. This 14-week course aims to introduce students to digital image analysis in the context of life sciences. We will cover the following topics:- Digital image data representations, formats, metadata- Image manipulation- Macro and script creation- Filtering, linear, non-linear, morphological- Segmentation- Regions of interest- Image stitching- Image visualisation- Data extraction and representation- Image deconvolution and denoising- Machine learning Each topic will have a strong emphasis on good practices and will be followed by exercises to be handed out at the next session. Exercises will involve the creation of macros or scripts to reach a defined goal. The exercises are to be completed as autonomous homework, outside of lecture hours. Keywords Biology, Image Processing, Microscopy, ImageJ, FIJI, Macros, Data, Segmentation,Filtering Visualisation Open so Assessment methods Continuous Multiple'}\n",
      "{'TEM': 1, 'imagej': 2, 'FIJI': 2, 'intends': 1, 'teach': 1, 'image': 12, 'processing': 3, 'strong': 2, 'emphasis': 2, 'application': 1, 'life': 3, 'science': 3, 'idea': 1, 'enable': 2, 'participant': 1, 'solve': 1, 'question': 1, 'workflow': 2, 'independently': 1, 'content': 1, 'decade': 1, 'arising': 1, 'microscope': 1, 'qualitative': 1, 'support': 1, 'scientific': 1, 'evidence': 1, 'quantitative': 1, 'resource': 1, 'obtain': 2, 'good': 2, 'quality': 1, 'data': 5, 'digital': 3, 'photograph': 1, 'western': 1, 'blot': 1, 'slice': 1, 'multichannel': 1, 'confocal': 1, 'timelapse': 1, 'stack': 1, 'scientist': 1, 'understand': 1, 'underlying': 1, 'process': 1, 'leading': 1, 'extracted': 1, 'information': 1, 'similar': 1, 'importance': 1, 'software': 1, 'make': 1, 'package': 1, 'opensource': 1, 'tool': 1, 'ensure': 1, 'maximum': 1, 'reproducibility': 1, 'protocol': 1, 'transfer': 1, 'analysis': 3, 'pipeline': 1, 'span': 1, 'week': 3, 'h30': 1, 'lecture': 2, 'exercise': 4, 'complete': 1, 'student': 3, 'establish': 1, 'autonomously': 1, 'note': 1, 'open': 2, 'max': 1, 'selected': 1, 'organizer': 1, 'aim': 1, 'introduce': 1, 'context': 1, 'cover': 1, 'topic': 2, 'representation': 2, 'format': 1, 'metadata': 1, 'manipulation': 1, 'macro': 3, 'script': 2, 'creation': 2, 'filtering': 2, 'linear': 1, 'nonlinear': 1, 'morphological': 1, 'segmentation': 2, 'region': 1, 'interest': 1, 'stitching': 1, 'visualisation': 2, 'extraction': 1, 'deconvolution': 1, 'denoising': 1, 'machine': 1, 'learning': 1, 'practice': 1, 'handed': 1, 'session': 1, 'involve': 1, 'reach': 1, 'defined': 1, 'goal': 1, 'completed': 1, 'autonomous': 1, 'homework': 1, 'hour': 1, 'keywords': 1, 'biology': 1, 'microscopy': 1, 'assessment': 1, 'method': 1, 'continuous': 1, 'multiple': 1}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'BIO-695': {'FIJI': 2,\n",
       "  'TEM': 1,\n",
       "  'aim': 1,\n",
       "  'analysis': 3,\n",
       "  'application': 1,\n",
       "  'arising': 1,\n",
       "  'assessment': 1,\n",
       "  'autonomous': 1,\n",
       "  'autonomously': 1,\n",
       "  'biology': 1,\n",
       "  'blot': 1,\n",
       "  'complete': 1,\n",
       "  'completed': 1,\n",
       "  'confocal': 1,\n",
       "  'content': 1,\n",
       "  'context': 1,\n",
       "  'continuous': 1,\n",
       "  'cover': 1,\n",
       "  'creation': 2,\n",
       "  'data': 5,\n",
       "  'decade': 1,\n",
       "  'deconvolution': 1,\n",
       "  'defined': 1,\n",
       "  'denoising': 1,\n",
       "  'digital': 3,\n",
       "  'emphasis': 2,\n",
       "  'enable': 2,\n",
       "  'ensure': 1,\n",
       "  'establish': 1,\n",
       "  'evidence': 1,\n",
       "  'exercise': 4,\n",
       "  'extracted': 1,\n",
       "  'extraction': 1,\n",
       "  'filtering': 2,\n",
       "  'format': 1,\n",
       "  'goal': 1,\n",
       "  'good': 2,\n",
       "  'h30': 1,\n",
       "  'handed': 1,\n",
       "  'homework': 1,\n",
       "  'hour': 1,\n",
       "  'idea': 1,\n",
       "  'image': 12,\n",
       "  'imagej': 2,\n",
       "  'importance': 1,\n",
       "  'independently': 1,\n",
       "  'information': 1,\n",
       "  'intends': 1,\n",
       "  'interest': 1,\n",
       "  'introduce': 1,\n",
       "  'involve': 1,\n",
       "  'keywords': 1,\n",
       "  'leading': 1,\n",
       "  'learning': 1,\n",
       "  'lecture': 2,\n",
       "  'life': 3,\n",
       "  'linear': 1,\n",
       "  'machine': 1,\n",
       "  'macro': 3,\n",
       "  'make': 1,\n",
       "  'manipulation': 1,\n",
       "  'max': 1,\n",
       "  'maximum': 1,\n",
       "  'metadata': 1,\n",
       "  'method': 1,\n",
       "  'microscope': 1,\n",
       "  'microscopy': 1,\n",
       "  'morphological': 1,\n",
       "  'multichannel': 1,\n",
       "  'multiple': 1,\n",
       "  'nonlinear': 1,\n",
       "  'note': 1,\n",
       "  'obtain': 2,\n",
       "  'open': 2,\n",
       "  'opensource': 1,\n",
       "  'organizer': 1,\n",
       "  'package': 1,\n",
       "  'participant': 1,\n",
       "  'photograph': 1,\n",
       "  'pipeline': 1,\n",
       "  'practice': 1,\n",
       "  'process': 1,\n",
       "  'processing': 3,\n",
       "  'protocol': 1,\n",
       "  'qualitative': 1,\n",
       "  'quality': 1,\n",
       "  'quantitative': 1,\n",
       "  'question': 1,\n",
       "  'reach': 1,\n",
       "  'region': 1,\n",
       "  'representation': 2,\n",
       "  'reproducibility': 1,\n",
       "  'resource': 1,\n",
       "  'science': 3,\n",
       "  'scientific': 1,\n",
       "  'scientist': 1,\n",
       "  'script': 2,\n",
       "  'segmentation': 2,\n",
       "  'selected': 1,\n",
       "  'session': 1,\n",
       "  'similar': 1,\n",
       "  'slice': 1,\n",
       "  'software': 1,\n",
       "  'solve': 1,\n",
       "  'span': 1,\n",
       "  'stack': 1,\n",
       "  'stitching': 1,\n",
       "  'strong': 2,\n",
       "  'student': 3,\n",
       "  'support': 1,\n",
       "  'teach': 1,\n",
       "  'timelapse': 1,\n",
       "  'tool': 1,\n",
       "  'topic': 2,\n",
       "  'transfer': 1,\n",
       "  'underlying': 1,\n",
       "  'understand': 1,\n",
       "  'visualisation': 2,\n",
       "  'week': 3,\n",
       "  'western': 1,\n",
       "  'workflow': 2}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_course = courses[1]\n",
    "bagOfWords = {}\n",
    "#globalBagOfWord = {}\n",
    "#getBagOfWords(courses[1]['description'])\n",
    "print(test_course)\n",
    "bow = getBagOfWords(test_course['description'])\n",
    "bagOfWords[test_course['courseId']] = bow\n",
    "#mergeBow(globalBagOfWord,bow)\n",
    "print(bow)\n",
    "dict(sorted(bagOfWords.items(), key=itemgetter(1), reverse=True)[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'interconnect': 3,\n",
       " 'postprimary': 3,\n",
       " 'inaction': 3,\n",
       " 'others6': 3,\n",
       " 'mechanosensory': 3,\n",
       " 'hyperelliptic': 3,\n",
       " 'FDD': 3,\n",
       " 'WINGS': 3,\n",
       " 'FRP': 3,\n",
       " 'valentine': 3,\n",
       " 'curable': 3,\n",
       " 'epifourier': 3,\n",
       " 'applications10': 3,\n",
       " 'emblematic': 3,\n",
       " 'microdispersed': 3,\n",
       " 'propagator': 3,\n",
       " 'photopolymers': 3,\n",
       " 'equivallent': 3,\n",
       " 'adaption': 3,\n",
       " 'aidistributed': 3,\n",
       " 'techniquesformulate': 3,\n",
       " 'gyromagnetic': 3,\n",
       " 'perl': 3,\n",
       " 'brief': 3,\n",
       " 'refreshment': 3,\n",
       " 'poroelasticity': 3,\n",
       " 'metaloxide': 3,\n",
       " 'ah20identify': 3,\n",
       " 'kirchoffs': 3,\n",
       " 'lawson': 3,\n",
       " 'EE332': 3,\n",
       " 'BBC': 3,\n",
       " 'skundin': 3,\n",
       " 'oksana': 3,\n",
       " 'decorrelation': 3,\n",
       " 'FINAL': 3,\n",
       " 'NEM': 3,\n",
       " 'transcriptase': 3,\n",
       " '2B': 3,\n",
       " 'processmicrostructure': 3,\n",
       " 'pip3signaling': 3,\n",
       " 'sectoral': 3,\n",
       " 'nozzle': 3,\n",
       " 'vestibular': 3,\n",
       " 'EPMAWDX': 3,\n",
       " 'light42': 3,\n",
       " 'dijkstras': 3,\n",
       " 'T3': 3,\n",
       " 'spontenaous': 3,\n",
       " 'URL': 3,\n",
       " 'multitemporal': 3,\n",
       " 'principles3': 3,\n",
       " 'hamming': 3,\n",
       " 'httpvectorcom': 3,\n",
       " 'orthopaedic': 3,\n",
       " 'subunit': 3,\n",
       " 'materialsapplications': 3,\n",
       " 'firouzeh': 3,\n",
       " 'vivian': 3,\n",
       " 'yearend': 3,\n",
       " 'incrementaliterative': 3,\n",
       " 'labexercise': 3,\n",
       " 'absorber': 3,\n",
       " 'thickeningstabilisationdewatering': 3,\n",
       " 'gmmbased': 3,\n",
       " 'reception': 3,\n",
       " 'diagrammes': 3,\n",
       " 'facilitates': 3,\n",
       " 'electrolysis': 3,\n",
       " 'DOF': 3,\n",
       " 'VQ': 3,\n",
       " 'misfolding': 3,\n",
       " 'twodimensionnal': 3,\n",
       " 'widespread': 3,\n",
       " 'CLI': 3,\n",
       " 'interactions2': 3,\n",
       " 'intensification': 3,\n",
       " 'gamal': 3,\n",
       " 'synchrone': 3,\n",
       " 'qualtiy': 3,\n",
       " 'electronical': 3,\n",
       " 'pharmabiomedical': 3,\n",
       " 'droop': 3,\n",
       " 'stœchiometry': 3,\n",
       " 'potable': 3,\n",
       " 'extemporaneous': 3,\n",
       " 'biology4': 3,\n",
       " 'incompatibility': 3,\n",
       " 'LTE': 3,\n",
       " 'internalized': 3,\n",
       " 'nonnormal': 3,\n",
       " 'draining': 3,\n",
       " 'leak': 3,\n",
       " 'owing': 3,\n",
       " 'huron': 3,\n",
       " 'palmer': 3,\n",
       " 'originality': 3,\n",
       " 'EBIC': 3,\n",
       " 'williams': 3,\n",
       " 'TCM': 3,\n",
       " 'missionoriented': 3,\n",
       " 'krane': 3,\n",
       " 'pressurized': 3,\n",
       " 'triggered': 3,\n",
       " 'fretbased': 3,\n",
       " 'dissolved': 3,\n",
       " 'proposal25': 3,\n",
       " 'serveral': 3,\n",
       " 'dcampepflch': 3,\n",
       " 'progesterone': 3,\n",
       " 'smoothness': 3,\n",
       " 'DRAM': 3,\n",
       " '443b': 3,\n",
       " 'aroma': 3,\n",
       " 'spoken': 3,\n",
       " 'developping': 3,\n",
       " 'spirale': 3,\n",
       " 'questionandanswer': 3,\n",
       " 'crosscoupled': 3,\n",
       " 'gas4': 3,\n",
       " 'caractères': 3,\n",
       " 'trained': 3,\n",
       " 'technologies2': 3,\n",
       " 'relaxation8': 3,\n",
       " 'ABCD': 3,\n",
       " 'diods': 3,\n",
       " 'thegrading': 3,\n",
       " 'stability6': 3,\n",
       " 'langrangian': 3,\n",
       " 'inbetween': 3,\n",
       " 'transformative': 3,\n",
       " 'noncircular': 3,\n",
       " 'recommeded': 3,\n",
       " 'microscopist': 3,\n",
       " 'abroad': 3,\n",
       " 'transitions32': 3,\n",
       " 'internationally': 3,\n",
       " 'sessin': 3,\n",
       " 'strategies55': 3,\n",
       " 'coupledoscillators': 3,\n",
       " 'resourceefficient': 3,\n",
       " 'indicative': 3,\n",
       " 'ragone': 3,\n",
       " 'opacity': 3,\n",
       " 'faded': 3,\n",
       " 'analyzeand': 3,\n",
       " 'filgueiras': 3,\n",
       " 'microarchitectures': 3,\n",
       " 'imagessimulate': 3,\n",
       " 'andmagnetization': 3,\n",
       " 'kim': 3,\n",
       " 'originating': 3,\n",
       " 'tridiagonal': 3,\n",
       " 'appoaches': 3,\n",
       " 'normed': 3,\n",
       " 'epistemetechne': 3,\n",
       " 'SURFACE': 3,\n",
       " 'triggering': 3,\n",
       " 'nonchemosensory': 3,\n",
       " 'buckingham': 3,\n",
       " 'tamas': 3,\n",
       " 'offboardonboard': 3,\n",
       " 'enviroment': 3,\n",
       " 'develope': 3,\n",
       " 'pressurised': 3,\n",
       " 'intention': 3,\n",
       " 'freelyjointed': 3,\n",
       " 'hyphenation': 3,\n",
       " 'specialworkshops': 3,\n",
       " 'ETH': 3,\n",
       " 'measurementcharacterization': 3,\n",
       " 'E8': 3,\n",
       " 'wesolowski': 3,\n",
       " 'alteration': 3,\n",
       " 'solvable': 3,\n",
       " 'snells': 3,\n",
       " 'onotation': 3,\n",
       " 'jura': 3,\n",
       " 'datasheets': 3,\n",
       " 'magnification': 3,\n",
       " 'lasers2': 3,\n",
       " 'labcourse': 3,\n",
       " 'ther': 3,\n",
       " 'nonspatial': 3,\n",
       " 'satisfied': 3,\n",
       " 'workability': 3,\n",
       " 'immobilization': 3,\n",
       " 'channels5': 3,\n",
       " 'inherited': 3,\n",
       " 'gasphase': 3,\n",
       " 'hexa': 3,\n",
       " 'crowd': 3,\n",
       " 'persona': 3,\n",
       " 'orlando': 3,\n",
       " 'biothermodynamics': 3,\n",
       " 'yearly': 3,\n",
       " 'UART': 3,\n",
       " 'interfaces5': 3,\n",
       " 'fleisher': 3,\n",
       " 'crustal': 3,\n",
       " 'rnabased': 3,\n",
       " 'soap': 3,\n",
       " 'disassemble': 3,\n",
       " 'subtypes': 3,\n",
       " 'sampler': 3,\n",
       " 'biochemicalbiological': 3,\n",
       " 'problems12': 3,\n",
       " 'vant': 3,\n",
       " 'geochemistry': 3,\n",
       " 'inadequate': 3,\n",
       " 'partnership': 3,\n",
       " 'marginalization': 3,\n",
       " 'wetland': 3,\n",
       " 'stratification': 3,\n",
       " 'behave': 3,\n",
       " 'munarin': 3,\n",
       " 'etc5': 3,\n",
       " 'dedication': 3,\n",
       " 'supplied': 3,\n",
       " 'vera': 3,\n",
       " 'synthesis3': 3,\n",
       " 'attributed': 3,\n",
       " 'laureate': 3,\n",
       " 'valuetheorem': 3,\n",
       " 'wormlike': 3,\n",
       " 'integrability': 3,\n",
       " 'fitout': 3,\n",
       " 'PSK': 3,\n",
       " 'mezzanine': 3,\n",
       " 'SANS': 3,\n",
       " 'VSEPR': 3,\n",
       " 'electromotive': 3,\n",
       " 'factorial': 3,\n",
       " 'embeded': 3,\n",
       " 'httpmoodleepflchcourseviewphpid3671': 3,\n",
       " 'rootcause': 3,\n",
       " 'rearranged': 3,\n",
       " 'majeurs': 3,\n",
       " 'metabolism7': 3,\n",
       " 'caseof': 3,\n",
       " 'tree4': 3,\n",
       " 'layering': 3,\n",
       " 'CSR': 3,\n",
       " 'EXAMINATION': 3,\n",
       " 'ksat12': 3,\n",
       " 'refers': 3,\n",
       " 'correspondent': 3,\n",
       " 'fusion2': 3,\n",
       " 'courjault': 3,\n",
       " 'freeradical': 3,\n",
       " 'httpwwwdouloscomknowhowvhdldesignersguidehttpwwwdouloscomknowhowsysveriloghttpwwwdouloscomknowhowsystemc': 3,\n",
       " 'landolt': 3,\n",
       " 'accept': 3,\n",
       " 'patienttherapistoriented': 3,\n",
       " 'httpwwwsp4commorg': 3,\n",
       " 'autonomy': 3,\n",
       " 'constructed': 3,\n",
       " 'mucosal': 3,\n",
       " 'analized': 3,\n",
       " 'isues': 3,\n",
       " 'andregulation': 3,\n",
       " 'workprinciple': 3,\n",
       " 'monomer': 3,\n",
       " 'internalizing': 3,\n",
       " 'exploratory': 3,\n",
       " 'forthe': 3,\n",
       " 'expects': 3,\n",
       " 'deembedding': 3,\n",
       " 'rewritingbased': 3,\n",
       " 'photoresists': 3,\n",
       " 'Q': 3,\n",
       " 'nanomanipulation': 3,\n",
       " 'CS107': 3,\n",
       " 'espionage': 3,\n",
       " 'ah2describe': 3,\n",
       " 'openness': 3,\n",
       " 'semaphor': 3,\n",
       " 'untapped': 3,\n",
       " 'ar201c': 3,\n",
       " 'photons2': 3,\n",
       " 'wavlength': 3,\n",
       " 'edpsciences': 3,\n",
       " 'potentiality': 3,\n",
       " 'excatrhedra': 3,\n",
       " 'highpass': 3,\n",
       " 'zvi': 3,\n",
       " 'celine': 3,\n",
       " 'lasers41': 3,\n",
       " 'mesurands': 3,\n",
       " 'FSM': 3,\n",
       " 'MICRO330': 3,\n",
       " 'kepler': 3,\n",
       " 'generationrecombination': 3,\n",
       " 'membersa': 3,\n",
       " 'feedforward': 3,\n",
       " 'HEPA': 3,\n",
       " 'siting': 3,\n",
       " 'chemosphere': 3,\n",
       " 'prob': 3,\n",
       " 'dominated': 3,\n",
       " 'twelve': 3,\n",
       " 'ah29perform': 3,\n",
       " 'itraqtmt': 3,\n",
       " 'aronson': 3,\n",
       " 'labvisits': 3,\n",
       " 'oceanography': 3,\n",
       " 'feasible': 3,\n",
       " 'tractable': 3,\n",
       " 'toxicological': 3,\n",
       " 'examples4th': 3,\n",
       " 'filtres': 3,\n",
       " 'counterdiffusion': 3,\n",
       " 'divertor': 3,\n",
       " 'ah11link': 3,\n",
       " 'blackandwhite': 3,\n",
       " 'profitable': 3,\n",
       " 'E19': 3,\n",
       " 'netword': 3,\n",
       " 'motility3': 3,\n",
       " 'opv': 3,\n",
       " 'economicsâ\\x80\\x99': 3,\n",
       " 'quartic': 3,\n",
       " 'inconsistent': 3,\n",
       " 'spacetimeefficient': 3,\n",
       " 'remedy': 3,\n",
       " 'neufville': 3,\n",
       " 'massproduction': 3,\n",
       " 'monument': 3,\n",
       " 'basketweave': 3,\n",
       " 'writen': 3,\n",
       " 'biopharmaceuticals': 3,\n",
       " 'pattaroni': 3,\n",
       " 'newmann': 3,\n",
       " 'asbuilt': 3,\n",
       " 'availability3': 3,\n",
       " 'pressuriseg': 3,\n",
       " 'spectroscopythermal': 3,\n",
       " 'CAVE': 3,\n",
       " 'HETP': 3,\n",
       " 'PEAQ': 3,\n",
       " 'nanoscaled': 3,\n",
       " 'twopage': 3,\n",
       " 'E12': 3,\n",
       " 'electronicstructure': 3,\n",
       " 'acqusition': 3,\n",
       " 'backanalysis': 3,\n",
       " 'rf': 3,\n",
       " 'agentslanthanidesactinidescoordination': 3,\n",
       " 'multijunction': 3,\n",
       " 'INO': 3,\n",
       " 'anddevelop': 3,\n",
       " 'interafce': 3,\n",
       " 'AISC34110': 3,\n",
       " 'artery': 3,\n",
       " 'scopus': 3,\n",
       " 'subtransmission': 3,\n",
       " 'bruus': 3,\n",
       " 'mediumsized': 3,\n",
       " 'freqon': 3,\n",
       " 'stressful': 3,\n",
       " 'pharma': 3,\n",
       " 'ah1link': 3,\n",
       " 'propagationiv': 3,\n",
       " 'pressurevelocity': 3,\n",
       " 'counterintuitive': 3,\n",
       " 'ah11state': 3,\n",
       " 'firstyear': 3,\n",
       " 'eyer': 3,\n",
       " 'transistorlevel': 3,\n",
       " 'oversampling': 3,\n",
       " 'geneva9': 3,\n",
       " 'transferfunction': 3,\n",
       " 'fondamentaux': 3,\n",
       " 'looked': 3,\n",
       " 'BIOENG437': 3,\n",
       " 'proximity': 3,\n",
       " 'KPI': 3,\n",
       " 'symmetryrelated': 3,\n",
       " 'multiview': 3,\n",
       " 'highspecialized': 3,\n",
       " 'conditions2': 3,\n",
       " 'cp6choose': 3,\n",
       " 'JMLR': 3,\n",
       " 'educate': 3,\n",
       " 'CH405': 3,\n",
       " 'entanglement': 3,\n",
       " 'twinned': 3,\n",
       " 'counterpart': 3,\n",
       " 'httpsvepflchfilescontentsitessvnew2filessharedisrecpdfgraceisrec20lectures20202016pdf': 3,\n",
       " 'CS101': 3,\n",
       " 'practicaloriented': 3,\n",
       " 'introdution': 3,\n",
       " 'ignition': 3,\n",
       " 'VLIW': 3,\n",
       " 'pack': 3,\n",
       " 'gruettermri': 3,\n",
       " 'AMRWB': 3,\n",
       " 'hypersensitive': 3,\n",
       " 'SNOM': 3,\n",
       " 'bigio': 3,\n",
       " 'snowpackformulate': 3,\n",
       " 'lumped': 3,\n",
       " 'UMP': 3,\n",
       " 'calculated': 3,\n",
       " 'elicit': 3,\n",
       " 'hernia': 3,\n",
       " 'abvariable': 3,\n",
       " 'compiled': 3,\n",
       " 'majority': 3,\n",
       " 'inorganc': 3,\n",
       " 'feedbacksolving': 3,\n",
       " 'enact': 3,\n",
       " 'predatorprey': 3,\n",
       " 'stressed': 3,\n",
       " 'photoconductors': 3,\n",
       " 'PGP': 3,\n",
       " 'corentin': 3,\n",
       " 'synthesizing': 3,\n",
       " 'csi': 3,\n",
       " 'diagnostic5': 3,\n",
       " 'collaborates': 3,\n",
       " 'B11': 3,\n",
       " 'intractable': 3,\n",
       " 'chisquared': 3,\n",
       " 'interfirm': 3,\n",
       " 'datajudge': 3,\n",
       " 'preparatory': 3,\n",
       " 'metalcatalysis': 3,\n",
       " 'timedependant': 3,\n",
       " 'WSL': 3,\n",
       " 'JFA': 3,\n",
       " 'vein': 3,\n",
       " 'borrowed': 3,\n",
       " 'paralell': 3,\n",
       " 'interconnectionsynchronization': 3,\n",
       " 'scien\\xadti\\xadfic': 3,\n",
       " 'realizing': 3,\n",
       " 'lietrature': 3,\n",
       " 'turbo': 3,\n",
       " 'bioremediation': 3,\n",
       " 'CAE': 3,\n",
       " 'observable': 3,\n",
       " 'taleba': 3,\n",
       " 'significantly': 3,\n",
       " 'respiration': 3,\n",
       " 'vol38': 3,\n",
       " 'flagellum': 3,\n",
       " 'eschenmoser': 3,\n",
       " 'prion': 3,\n",
       " 'strait': 3,\n",
       " 'measurments': 3,\n",
       " 'metabolism8': 3,\n",
       " 'berlindr': 3,\n",
       " 'committee': 3,\n",
       " 'beamforming': 3,\n",
       " 'tablet': 3,\n",
       " 'norman': 3,\n",
       " 'altogether': 3,\n",
       " 'appendage': 3,\n",
       " 'writeoncereadmany': 3,\n",
       " 'ROI': 3,\n",
       " 'formalise': 3,\n",
       " 'precession': 3,\n",
       " 'buil': 3,\n",
       " 'httpwwwmitrplodzplevulectures': 3,\n",
       " 'suisse': 3,\n",
       " 'eppinger': 3,\n",
       " 'BRST': 3,\n",
       " 'elucidating': 3,\n",
       " 'cleaning': 3,\n",
       " 'hyphenated': 3,\n",
       " 'environomic': 3,\n",
       " 'micromechanisms': 3,\n",
       " 'demurtasa': 3,\n",
       " 'determinism': 3,\n",
       " 'ebeams': 3,\n",
       " 'referenceframe': 3,\n",
       " 'methods4': 3,\n",
       " 'wisconsin': 3,\n",
       " 'manfred': 3,\n",
       " 'manufacturingtechniques': 3,\n",
       " 'gpsspectral': 3,\n",
       " 'systèmesonchip': 3,\n",
       " 'nanolithography': 3,\n",
       " 'michler': 3,\n",
       " 'HENRY': 3,\n",
       " 'sessions1': 3,\n",
       " 'TCB': 3,\n",
       " 'intermediaries7': 3,\n",
       " 'allostery': 3,\n",
       " 'einsteinstokes': 3,\n",
       " 'interferomety': 3,\n",
       " 'preperation': 3,\n",
       " 'wildtype': 3,\n",
       " 'ivo': 3,\n",
       " 'persuasive': 3,\n",
       " 'helioseismology': 3,\n",
       " 'bandshift': 3,\n",
       " 'digaonalization': 3,\n",
       " 'doesnt': 3,\n",
       " 'gachet': 3,\n",
       " 'rayon': 3,\n",
       " 'lumetric': 3,\n",
       " 'dnabased': 3,\n",
       " 'yamabe': 3,\n",
       " 'enjoyability': 3,\n",
       " 'FCS': 3,\n",
       " 'frugal': 3,\n",
       " 'cysteine': 3,\n",
       " 'hydrostatics': 3,\n",
       " 'opentext': 3,\n",
       " 'arnell': 3,\n",
       " 'oksendal': 3,\n",
       " 'RESEARCH': 3,\n",
       " 'eye3': 3,\n",
       " 'transcriptional': 3,\n",
       " 'httplcvmwwwepflchcgdna': 3,\n",
       " 'viganò': 3,\n",
       " 'GGA8': 3,\n",
       " 'questioning': 3,\n",
       " 'collaborate': 3,\n",
       " 'lidaronchip': 3,\n",
       " 'liouvillespace': 3,\n",
       " 'diffraction41': 3,\n",
       " 'bilateral': 3,\n",
       " 'baum': 3,\n",
       " 'timer': 3,\n",
       " 'antifragility': 3,\n",
       " 'began': 3,\n",
       " 'unnatural': 3,\n",
       " 'channeling': 3,\n",
       " 'microct': 3,\n",
       " 'jantsch': 3,\n",
       " 'homeostasisregeneration': 3,\n",
       " 'DIBL': 3,\n",
       " 'socioeconomical': 3,\n",
       " 'MATH407': 3,\n",
       " 'MPEG21': 3,\n",
       " 'synchonization': 3,\n",
       " 'deployed': 3,\n",
       " 'operation10': 3,\n",
       " 'consulter': 3,\n",
       " 'STUDIO': 3,\n",
       " 'AO': 3,\n",
       " 'replacement': 3,\n",
       " 'stated': 3,\n",
       " 'prolog': 3,\n",
       " 'ultrashallow': 3,\n",
       " 'differentiationcommitment': 3,\n",
       " 'hysicochemical': 3,\n",
       " 'stoechiometric': 3,\n",
       " 'piezoelectrics5': 3,\n",
       " 'biomineralization6': 3,\n",
       " 'propopsed': 3,\n",
       " 'dorling': 3,\n",
       " 'roland': 3,\n",
       " 'tensentence': 3,\n",
       " 'insulator': 3,\n",
       " 'stemming': 3,\n",
       " 'evasion': 3,\n",
       " 'pleasure': 3,\n",
       " 'domaine': 3,\n",
       " 'polymers6': 3,\n",
       " 'ndphysique': 3,\n",
       " 'securitization': 3,\n",
       " 'incentives6': 3,\n",
       " 'dental': 3,\n",
       " 'biomechanical': 3,\n",
       " 'outperform': 3,\n",
       " 'electronnuclear': 3,\n",
       " 'collagen': 3,\n",
       " 'UMTSHSDPA': 3,\n",
       " 'canoe': 3,\n",
       " 'neurobiology': 3,\n",
       " 'AMP': 3,\n",
       " 'wishing': 3,\n",
       " 'tansforms': 3,\n",
       " 'behra': 3,\n",
       " 'triaxial': 3,\n",
       " 'transportcompensation': 3,\n",
       " 'diffuser': 3,\n",
       " 'electromechanics8': 3,\n",
       " 'librarian': 3,\n",
       " 'cole': 3,\n",
       " 'lactones': 3,\n",
       " 'soundness': 3,\n",
       " 'substates': 3,\n",
       " 'topi': 3,\n",
       " 'commont': 3,\n",
       " 'effects5': 3,\n",
       " 'CV': 3,\n",
       " 'nonreactive': 3,\n",
       " 'builiding': 3,\n",
       " 'relativevalue': 3,\n",
       " 'assenbly': 3,\n",
       " 'sequentialsynchronous': 3,\n",
       " 'id': 3,\n",
       " 'progressively': 3,\n",
       " 'predominantly': 3,\n",
       " 'pluralism': 3,\n",
       " 'sensing5': 3,\n",
       " 'MATH111': 3,\n",
       " 'modularity': 3,\n",
       " 'interspeaker': 3,\n",
       " 'lieb': 3,\n",
       " 'processes8': 3,\n",
       " 'surfacemodified': 3,\n",
       " 'clusteringd': 3,\n",
       " 'intramolecular': 3,\n",
       " 'subtraction': 3,\n",
       " 'duda': 3,\n",
       " 'type2': 3,\n",
       " 'insensibility': 3,\n",
       " 'modelsdatamining': 3,\n",
       " 'shell': 3,\n",
       " 'shortessay': 3,\n",
       " 'stiction': 3,\n",
       " 'REST': 3,\n",
       " 'SOSP': 3,\n",
       " 'ah25describe': 3,\n",
       " 'amplifying': 3,\n",
       " 'capitalist': 3,\n",
       " 'metallatedligands': 3,\n",
       " 'sheaf': 3,\n",
       " 'materialsphysics': 3,\n",
       " 'sizable': 3,\n",
       " 'creepand': 3,\n",
       " 'routinely': 3,\n",
       " 'pseudoreplications': 3,\n",
       " 'environnment': 3,\n",
       " 'recruitment': 3,\n",
       " 'decompostitions': 3,\n",
       " 'retrieve': 3,\n",
       " 'lightweight': 3,\n",
       " 'ck': 3,\n",
       " 'development5': 3,\n",
       " 'piston': 3,\n",
       " 'electricityproducing': 3,\n",
       " 'representativity': 3,\n",
       " 'touristic': 3,\n",
       " 'EFPL': 3,\n",
       " 'miniature': 3,\n",
       " 'telematics': 3,\n",
       " 'polarizers': 3,\n",
       " 'diego': 3,\n",
       " 'hematopoiesis': 3,\n",
       " 'ru': 3,\n",
       " 'orthopedics': 3,\n",
       " 'AISC35810': 3,\n",
       " 'publish': 3,\n",
       " 'EAWS': 3,\n",
       " 'perceiving': 3,\n",
       " 'cp5formulate': 3,\n",
       " 'BA2': 3,\n",
       " 'micro331': 3,\n",
       " 'analyzelisten': 3,\n",
       " 'intact': 3,\n",
       " 'pursuit': 3,\n",
       " 'perfusion': 3,\n",
       " 'corte': 3,\n",
       " 'abilty': 3,\n",
       " 'EVER': 3,\n",
       " 'inksubstrate': 3,\n",
       " 'tribocorrosion': 3,\n",
       " 'actuators6': 3,\n",
       " 'multiplicative': 3,\n",
       " 'opened': 3,\n",
       " 'cyclesspecifics': 3,\n",
       " 'bump': 3,\n",
       " 'thiophene': 3,\n",
       " 'alkaline': 3,\n",
       " 'examples3': 3,\n",
       " 'nanotoxicological': 3,\n",
       " 'TURCHI': 3,\n",
       " 'zippel': 3,\n",
       " 'ibn': 3,\n",
       " 'httpmoodleepflchcourseenrolphpid9371': 3,\n",
       " 'lo': 3,\n",
       " 'lebesgu': 3,\n",
       " 'runner': 3,\n",
       " 'aspectsphysical': 3,\n",
       " 'tropospheric': 3,\n",
       " 'ford': 3,\n",
       " 'clausing': 3,\n",
       " 'doubling': 3,\n",
       " 'phosphate': 3,\n",
       " 'anodic': 3,\n",
       " 'control40': 3,\n",
       " 'involvment': 3,\n",
       " 'peridoic': 3,\n",
       " 'learing': 3,\n",
       " 'ghetto': 3,\n",
       " 'regimeii': 3,\n",
       " 'biodegradable': 3,\n",
       " 'BROTTON': 3,\n",
       " 'replicating': 3,\n",
       " 'model7': 3,\n",
       " 'PWM': 3,\n",
       " 'psychometric': 3,\n",
       " 'DAY': 3,\n",
       " 'invertebrate': 3,\n",
       " 'editor': 3,\n",
       " 'steganography': 3,\n",
       " 'schemessuch': 3,\n",
       " 'FTCM': 3,\n",
       " 'partof': 3,\n",
       " 'characterise': 3,\n",
       " 'SETS': 3,\n",
       " 'byzantine': 3,\n",
       " 'tf': 3,\n",
       " 'HMMANN': 3,\n",
       " 'A5': 3,\n",
       " 'reductionof': 3,\n",
       " 'milli': 3,\n",
       " 'pioneer': 3,\n",
       " 'ethylene': 3,\n",
       " 'villette': 3,\n",
       " 'moses': 3,\n",
       " 'transitionsexcited': 3,\n",
       " 'modulationdemodulation': 3,\n",
       " 'veterinary': 3,\n",
       " 'ITU6': 3,\n",
       " 'biorefineries': 3,\n",
       " 'birkhäuser': 3,\n",
       " 'nanoelectronic': 3,\n",
       " 'neuchâtel': 3,\n",
       " 'httpmoodleepflchcourseviewphpid378': 3,\n",
       " 'infomation': 3,\n",
       " 'latestage': 3,\n",
       " 'effortful': 3,\n",
       " 'AKERMAN': 3,\n",
       " 'termal': 3,\n",
       " 'proliferation': 3,\n",
       " 'fetal': 3,\n",
       " 'poincares': 3,\n",
       " 'reservation': 3,\n",
       " 'uncontrolled': 3,\n",
       " 'broadly': 3,\n",
       " 'MSE231': 3,\n",
       " 'warmkessel': 3,\n",
       " 'splicing': 3,\n",
       " 'sanitaire': 3,\n",
       " 'operationsbe': 3,\n",
       " 'CJ': 3,\n",
       " 'crystals3': 3,\n",
       " 'nanotribology': 3,\n",
       " 'hardline': 3,\n",
       " 'semconductors': 3,\n",
       " 'coherence12': 3,\n",
       " 'configurable': 3,\n",
       " 'provenance': 3,\n",
       " 'eigenstates': 3,\n",
       " 'informationtheoretic': 3,\n",
       " 'andrew': 3,\n",
       " 'protected': 3,\n",
       " 'restauration': 3,\n",
       " 'liquefaction': 3,\n",
       " 'max40': 3,\n",
       " 'wormalds': 3,\n",
       " 'biotatalysis': 3,\n",
       " 'scraper': 3,\n",
       " 'embarassingly': 3,\n",
       " 'bloodvessel': 3,\n",
       " 'imaginarytime': 3,\n",
       " 'thematics': 3,\n",
       " 'discrimination': 3,\n",
       " 'ferroelastic': 3,\n",
       " 'disscued': 3,\n",
       " 'estimation4': 3,\n",
       " 'antagonism': 3,\n",
       " 'fluidics': 3,\n",
       " 'markup': 3,\n",
       " 'ah1describe': 3,\n",
       " 'bitinterleaved': 3,\n",
       " 'phosphorene': 3,\n",
       " 'july': 3,\n",
       " 'TRIBOLOGY': 3,\n",
       " 'highimpact': 3,\n",
       " 'decidability': 3,\n",
       " 'eaxam': 3,\n",
       " 'elastoplasticity': 3,\n",
       " 'kindersley': 3,\n",
       " 'backscatter': 3,\n",
       " 'lockfree': 3,\n",
       " 'metabolites3': 3,\n",
       " 'designflow': 3,\n",
       " 'dinner': 3,\n",
       " 'gross': 3,\n",
       " 'ciruits': 3,\n",
       " 'drainage': 3,\n",
       " 'tuned': 3,\n",
       " 'processes3rd': 3,\n",
       " 'ruines': 3,\n",
       " 'photochemisty': 3,\n",
       " 'copying': 3,\n",
       " 'MICRO102': 3,\n",
       " 'digest': 3,\n",
       " 'flask': 3,\n",
       " 'substances4': 3,\n",
       " 'MM': 3,\n",
       " 'practicebased': 3,\n",
       " 'repeat': 3,\n",
       " 'stokey': 3,\n",
       " 'linecamera': 3,\n",
       " 'derivatives8': 3,\n",
       " 'rpesentations': 3,\n",
       " 'BH': 3,\n",
       " 'pathological': 3,\n",
       " 'phyics': 3,\n",
       " 'argumentstensors': 3,\n",
       " 'classmate': 3,\n",
       " 'cooperative': 3,\n",
       " 'lifesciences': 3,\n",
       " 'surfing': 3,\n",
       " 'rockwell': 3,\n",
       " 'biotopes': 3,\n",
       " 'interscience': 3,\n",
       " 'sun': 3,\n",
       " 'coatingsubstrate': 3,\n",
       " 'gehrys': 3,\n",
       " 'properties6': 3,\n",
       " 'reactiondesorption': 3,\n",
       " 'nondissipative': 3,\n",
       " 'scientificgrade': 3,\n",
       " 'marsicano': 3,\n",
       " 'formalisation9': 3,\n",
       " 'enigma': 3,\n",
       " 'inpractical': 3,\n",
       " 'hystorically': 3,\n",
       " 'lola': 3,\n",
       " 'tsvs': 3,\n",
       " 'photosensor': 3,\n",
       " 'variableselection': 3,\n",
       " 'andmethods': 3,\n",
       " 'predesign': 3,\n",
       " 'merino': 3,\n",
       " 'compatible': 3,\n",
       " 'statespacecontrol': 3,\n",
       " 'regolators': 3,\n",
       " 'photomultipliers': 3,\n",
       " 'VC': 3,\n",
       " 'sébastien': 3,\n",
       " 'highq': 3,\n",
       " 'pricing4': 3,\n",
       " 'dropondemand': 3,\n",
       " 'PI': 3,\n",
       " 'AIAA20046012': 3,\n",
       " 'clausius': 3,\n",
       " 'santos': 3,\n",
       " 'mack': 3,\n",
       " 'drought': 3,\n",
       " 'demandsupply': 3,\n",
       " 'morton': 3,\n",
       " 'hayashi': 3,\n",
       " 'pinterest': 3,\n",
       " 'distributionevaluate': 3,\n",
       " 'cumputer': 3,\n",
       " 'solary': 3,\n",
       " 'auto': 3,\n",
       " 'equatorial': 3,\n",
       " 'unfired': 3,\n",
       " 'thermo': 3,\n",
       " 'fluidised': 3,\n",
       " 'sectional': 3,\n",
       " 'triazole': 3,\n",
       " 'OSVVM': 3,\n",
       " 'dbmsdesign': 3,\n",
       " 'passage': 3,\n",
       " 'knearest': 3,\n",
       " 'httpclickersepflchstudents': 3,\n",
       " 'wont': 3,\n",
       " 'brisken': 3,\n",
       " 'sodium': 3,\n",
       " 'practicesâ\\x80\\x9d': 3,\n",
       " 'portofolio': 3,\n",
       " '300CHF': 3,\n",
       " 'steepest': 3,\n",
       " 'avellan': 3,\n",
       " 'comapny': 3,\n",
       " 'actionsynthesize': 3,\n",
       " 'planning33': 3,\n",
       " 'unilever': 3,\n",
       " 'ceramicmetalglass': 3,\n",
       " 'fractional': 3,\n",
       " 'confinementiii': 3,\n",
       " 'metalcentered': 3,\n",
       " 'hamm': 3,\n",
       " 'nanosystem': 3,\n",
       " 'approriate': 3,\n",
       " 'articlebased': 3,\n",
       " 'transconductance': 3,\n",
       " 'zirconium': 3,\n",
       " 'DF': 3,\n",
       " 'lexical': 3,\n",
       " 'solidelectrolyte': 3,\n",
       " 'realizable': 3,\n",
       " 'flo': 3,\n",
       " 'lucas': 3,\n",
       " 'contintuity': 3,\n",
       " 'scènes': 3,\n",
       " 'moisture': 3,\n",
       " 'cytokine': 3,\n",
       " 'downconversion': 3,\n",
       " '36010AISC': 3,\n",
       " 'NIHS': 3,\n",
       " 'gnerative': 3,\n",
       " 'overcome': 3,\n",
       " 'abuse': 3,\n",
       " 'efficency': 3,\n",
       " 'lamination': 3,\n",
       " 'nonlti3': 3,\n",
       " 'landmarkbased': 3,\n",
       " 'superlattices': 3,\n",
       " 'restraint': 3,\n",
       " 'randomised': 3,\n",
       " 'hosted': 3,\n",
       " 'queen': 3,\n",
       " 'enbedded': 3,\n",
       " 'robotica': 3,\n",
       " 'chalcogenide': 3,\n",
       " 'writer': 3,\n",
       " 'mandelbrot': 3,\n",
       " 'cooccurrence': 3,\n",
       " 'semicustom': 3,\n",
       " 'biblio': 3,\n",
       " 'MICRO455': 3,\n",
       " 'macroscale': 3,\n",
       " 'phonetic': 3,\n",
       " 'actuating': 3,\n",
       " 'sommerfeld': 3,\n",
       " 'chrystallography': 3,\n",
       " 'quasar': 3,\n",
       " 'tenet': 3,\n",
       " 'robustly': 3,\n",
       " 'infusionmsms': 3,\n",
       " 'photostability4': 3,\n",
       " 'shelflife': 3,\n",
       " 'meibom': 3,\n",
       " 'positioning4': 3,\n",
       " 'hutchings': 3,\n",
       " 'cartographer': 3,\n",
       " 'andy': 3,\n",
       " 'mol': 3,\n",
       " 'materials5': 3,\n",
       " 'leadtime': 3,\n",
       " 'hardcore': 3,\n",
       " 'engaged': 3,\n",
       " 'MSE203': 3,\n",
       " 'limp': 3,\n",
       " 'artin': 3,\n",
       " 'lecures': 3,\n",
       " 'addons': 3,\n",
       " 'developme': 3,\n",
       " 'CS452': 3,\n",
       " 'demarchi': 3,\n",
       " 'esp': 3,\n",
       " 'haskell': 3,\n",
       " 'ah28analyze': 3,\n",
       " 'chaize': 3,\n",
       " 'und': 3,\n",
       " 'physicalchemical': 3,\n",
       " 'CBCM': 3,\n",
       " 'chargedipole': 3,\n",
       " 'publisher': 3,\n",
       " 'prototyped': 3,\n",
       " 'polyurethane': 3,\n",
       " 'conformal': 3,\n",
       " 'sony': 3,\n",
       " 'elementsmagnetic': 3,\n",
       " 'databasesearch': 3,\n",
       " 'hoare': 3,\n",
       " 'alain': 3,\n",
       " 'supplies4': 3,\n",
       " 'EE350': 3,\n",
       " 'meylan': 3,\n",
       " 'warren': 3,\n",
       " 'falling': 3,\n",
       " 'ah11work': 3,\n",
       " 'openly': 3,\n",
       " 'HUM417': 3,\n",
       " 'channels4': 3,\n",
       " 'portrait': 3,\n",
       " 'solidcatalyzed': 3,\n",
       " 'socioeconomis': 3,\n",
       " 'cholinergic': 3,\n",
       " 'inthe': 3,\n",
       " 'hoc': 3,\n",
       " 'PM': 3,\n",
       " 'rodogno': 3,\n",
       " 'deflection': 3,\n",
       " 'hemaotopoietic': 3,\n",
       " 'mut': 3,\n",
       " 'carbocyclization': 3,\n",
       " 'TPCIP': 3,\n",
       " 'dayweek': 3,\n",
       " 'corotational': 3,\n",
       " 'GPR': 3,\n",
       " 'parallelizing': 3,\n",
       " 'atherosclerotic': 3,\n",
       " 'transitors': 3,\n",
       " ...}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(globalBagOfWords,key=globalBagOfWords.get)\n",
    "dict(sorted(globalBagOfWords.items(), key=itemgetter(1), reverse=False)[:5000])\n",
    "# Maybe discard only the 3 most used words? since system and design are more specific than learning, student and system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test sample of courses (just the first 3)\n",
    "sampleCourses = [{'courseId': 'MSC-101',\n",
    "  'description': \"Here comes the sun, dudududu, here comes the sun and I say...\",\n",
    "  'name': 'The Beatles'},\n",
    "                 {'courseId': 'MSC-102',\n",
    "  'description': \"In an octupus's garden, in the sea. He'd let us in...\",\n",
    "  'name': 'The Beatles Too'},\n",
    "                 {'courseId': 'MSC-103',\n",
    "    'description': \"Born, to be wiiiiild dudududu\",\n",
    "    'name': 'Steppenwolf'}]\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'octupuss': 1, 'I': 1, 'sea': 1, 'hed': 1, 'sun': 2, 'dudududu': 1, 'garden': 1}\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "sampleGlobalBagOfWords = {}\n",
    "sampleGlobalBagOfWords = getGlobalBagOfWords(sampleCourses)\n",
    "print(sampleGlobalBagOfWords)\n",
    "print(sum(sampleGlobalBagOfWords.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'octupuss': 1, 'I': 1, 'sea': 1, 'hed': 1, 'sun': 2, 'dudududu': 1, 'garden': 1}\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "sampleBagOfWords = {}\n",
    "sampleGlobalBagOfWords = {}\n",
    "getBagOfWords(sampleCourses[1]['description'])\n",
    "for course in sampleCourses:\n",
    "    sampleBow = getBagOfWords(course['description'])\n",
    "    sampleBagOfWords[course['courseId']] = sampleBow\n",
    "    mergeBow(sampleGlobalBagOfWords,sampleBow)\n",
    "#test_course = sampleCourses.copy()\n",
    "#print(sampleBagOfWords)\n",
    "print(sampleGlobalBagOfWords)\n",
    "#print(sum(sampleBagOfWords.values()))\n",
    "print(sum(sampleGlobalBagOfWords.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#courses\n",
    "#globalBagOfWord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Term Document Matrix\n",
    "# We want a matrix where each row i is a word (among global bag of words) \n",
    "# and each column j is a document (among all courses)\n",
    "# tdm[i][j] = nb of occurences of term i in doc j\n",
    "\n",
    "def getTermDocMatrix(courses):\n",
    "    \n",
    "    # get global bag of word (for all courses combined) \n",
    "    globalBagOfWord = getGlobalBagOfWords(courses)\n",
    "    \n",
    "    # total number of terms\n",
    "    M = len(globalBagOfWord)\n",
    "\n",
    "    # total number of documents\n",
    "    N = len(courses)\n",
    "\n",
    "    termDocMatrix = np.zeros((M,N), dtype=np.int)\n",
    "\n",
    "    # Column index\n",
    "    docIndx = 0\n",
    "\n",
    "    for doc in courses: \n",
    "        bow = getBagOfWords(doc['description'])\n",
    "        # Row index\n",
    "        termIndx = 0\n",
    "        for word in globalBagOfWord.keys():\n",
    "            termDocMatrix[termIndx][docIndx] += bow.get(word, 0)\n",
    "            termIndx += 1\n",
    "        docIndx += 1\n",
    "    return termDocMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [2 0]\n",
      " [1 0]\n",
      " [0 1]]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'getTermFrequency' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-68a3395eaf2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetTermDocMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampleCourses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetTermFrequency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampleCourses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetImportance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampleCourses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetInverseDocFrequency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampleCourses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'getTermFrequency' is not defined"
     ]
    }
   ],
   "source": [
    "print(getTermDocMatrix(sampleCourses))\n",
    "print(getTermFrequency(sampleCourses))\n",
    "print(getImportance(sampleCourses))\n",
    "print(getInverseDocFrequency(sampleCourses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.        ,  4.        ,  0.52941176, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 2.        ,  1.66666667,  0.88235294, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  1.33333333,  0.94117647, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ..., \n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute fij (frequency of term i in doc j), à transformer en fonction?\n",
    "\n",
    "def getTermFrequency():\n",
    "    # get global bag of word (for all courses combined) \n",
    "    globalBagOfWord, bagOfWords = getGlobalBagOfWords()\n",
    "\n",
    "    totalWords = len(globalBagOfWord)\n",
    "    totalCourses = len(courses)\n",
    "\n",
    "    f = np.zeros((totalWords,totalCourses),dtype=np.double)\n",
    "\n",
    "    docIndx = 0\n",
    "    for bow in bagOfWords.values():\n",
    "        wordIndx = 0\n",
    "        for word in bow:\n",
    "            f[wordIndx][docIndx] = bow.get(word,0) / len(bow) \n",
    "            wordIndx += 1\n",
    "        docIndx += 1\n",
    "    return f\n",
    "getTermFrequency()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute TFij \"importance of word i in doc j\"\n",
    "def getImportance(courses):\n",
    "    # get global bag of word (for all courses combined) \n",
    "    globalBagOfWord = getGlobalBagOfWords(courses)\n",
    "\n",
    "    totalWords = len(globalBagOfWord)\n",
    "    totalCourses = len(courses)\n",
    "    \n",
    "    TF = np.zeros((totalWords,totalCourses),dtype=np.double)\n",
    "    maxWordOfDoc = [\"\"]*totalCourses\n",
    "    \n",
    "    # Compute term frequency\n",
    "    f = np.zeros((totalWords,totalCourses),dtype=np.double)\n",
    "    f = getTermFrequency(courses)\n",
    "    \n",
    "    # Find the number of occurences of the most used words in every document\n",
    "    docIndx = 0\n",
    "    for doc in courses:\n",
    "        bow = getBagOfWords(doc['description'])\n",
    "        maxWordOfDoc[docIndx] = list(bow.values())[np.argmax(f[docIndx])]\n",
    "        docIndx += 1\n",
    "\n",
    "    # Compute TF\n",
    "    # Note: on a pas besoin d'itérer sur toutes les lignes, juste les colonnes mais j'arrive pas à utiliser \n",
    "    # la fonction np.apply_along_axis en passant une fonction qui doit connaitre l'indice de la ligne\n",
    "    for i in range(totalWords):\n",
    "        for j in range(totalCourses):\n",
    "            TF[i][j] = f[i][j] / maxWordOfDoc[j]\n",
    "    return TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inverse Document Frequency : IDF\n",
    "# Compute n[i] = nb of documents where word i occurs at least once\n",
    "def getInverseDocFrequency(courses):\n",
    "    # get global bag of word (for all courses combined) \n",
    "    globalBagOfWord = getGlobalBagOfWords(courses)\n",
    "    totalWords = len(globalBagOfWord)\n",
    "    totalCourses = len(courses)\n",
    "\n",
    "    # Compute term frequency\n",
    "    f = np.zeros((totalWords,totalCourses),dtype=np.double)\n",
    "    f = getTermFrequency(courses)\n",
    "    \n",
    "    n = np.zeros((totalWords),dtype=np.int)\n",
    "    IDF = np.zeros((totalWords),dtype=np.double)\n",
    "\n",
    "    for i in range(totalWords):\n",
    "        for j in range(totalCourses):\n",
    "            if(f[i][j] != 0):\n",
    "                n[i] += 1\n",
    "        if(n[i] == 0):\n",
    "            IDF[i] = 0\n",
    "        else:\n",
    "            IDF[i] = -math.log2(n[i]/totalCourses)\n",
    "    return IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.   -0.  ]\n",
      " [-0.   -0.  ]\n",
      " [-0.   -0.  ]\n",
      " [ 0.    0.25]\n",
      " [ 0.    0.  ]\n",
      " [ 0.    0.  ]\n",
      " [ 0.    0.  ]]\n"
     ]
    }
   ],
   "source": [
    "# Compute TF-IDF score\n",
    "\n",
    "# get global bag of word (for all courses combined) \n",
    "globalBagOfWord = getGlobalBagOfWords(sampleCourses)\n",
    "\n",
    "totalWords = len(globalBagOfWord)\n",
    "totalCourses = len(sampleCourses)\n",
    "\n",
    "# Get term frequency\n",
    "TF = np.zeros((totalWords,totalCourses),dtype=np.double)\n",
    "TF = getImportance(sampleCourses)\n",
    "\n",
    "# Get inverse document frequency\n",
    "IDF = np.zeros((totalWords),dtype=np.double)\n",
    "IDF = getInverseDocFrequency(sampleCourses)\n",
    "    \n",
    "TFIDF = np.zeros((totalWords,totalCourses),dtype=np.double)\n",
    "\n",
    "# Compute TF-IDF\n",
    "for i in range(totalWords):\n",
    "    for j in range(totalCourses):\n",
    "        TFIDF[i][j] = TF[i][j] * IDF[i]\n",
    "print(TFIDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the slides:\n",
    "\n",
    "N = total number of documents\n",
    "\n",
    "f[i][j] nb of occurrences of word 𝑖 in doc 𝑗, so bagOfWord(j)[i]\n",
    "\n",
    "tf[i][j] = f[i][j] / max_k f[k][j]\n",
    "\n",
    "idf[i] = -log_2(number of documents where word i occurs at least once / N)\n",
    "\n",
    "tfidf[i][j] = tf[i][j] * idf[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sim(doc1, doc2):\n",
    "    return np.dot(np.transpose(doc1), doc2) / (np.linalg.norm(doc1) * np.linalg.norm(doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'The Beatles', 'description': 'Here comes the sun, dudududu, here comes the sun and I say...', 'courseId': 'MSC-101'}, {'name': 'The Beatles Too', 'description': \"In an octupus's garden, in the sea. He'd let us in...\", 'courseId': 'MSC-102'}, {'name': 'Steppenwolf', 'description': 'Born, to be wiiiiild dudududu', 'courseId': 'MSC-103'}]\n",
      "[[0 1 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 1 0]\n",
      " [2 0 0]\n",
      " [1 0 1]\n",
      " [0 0 1]\n",
      " [0 1 0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.23570226039551587"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sampleCourses)\n",
    "tdc = getTermDocMatrix(sampleCourses)\n",
    "print(tdc)\n",
    "sim(tdc[:,0], tdc[:,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.2: Term-document matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2]\n",
      " [ 3  4]\n",
      " [ 5  6]\n",
      " [ 7  8]\n",
      " [ 9 10]]\n",
      "[[    10     10]\n",
      " [   100    100]\n",
      " [  1000   1000]\n",
      " [ 10000  10000]\n",
      " [100000 100000]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[     10,      20],\n",
       "       [    300,     400],\n",
       "       [   5000,    6000],\n",
       "       [  70000,   80000],\n",
       "       [ 900000, 1000000]])"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array(((1,2),(3,4),(5,6),(7,8),(9,10)))\n",
    "print(a)\n",
    "b = np.tile(np.array((10,100,1000,10000,100000)),(2,1)).T\n",
    "print(b)\n",
    "a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "globalBagOfWords, bagOfWords = getGlobalBagOfWords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "numTerms = len(globalBagOfWords.keys())\n",
    "numCourses = len(bagOfWords.keys())\n",
    "termsOrder = list(enumerate(globalBagOfWords.keys()))\n",
    "coursesOrder = list(enumerate(bagOfWords.keys()))\n",
    "idx2Term = {i[0]: i[1] for i in termsOrder}\n",
    "term2Idx = {v: k for k,v in idx2Term.items()}\n",
    "idx2Course = {i[0]: i[1] for i in coursesOrder}\n",
    "course2Idx = {v: k for k,v in idx2Course.items()}\n",
    "np.save('idx2Term', idx2Term)\n",
    "np.save('term2Idx', term2Idx)\n",
    "np.save('idx2Course', idx2Course)\n",
    "np.save('course2Idx', course2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf = np.zeros((numTerms,numCourses))\n",
    "overallFreq = np.zeros(numTerms)\n",
    "for courseIdx, course in coursesOrder:\n",
    "    if(len(bagOfWords[course]) == 0):\n",
    "        continue\n",
    "    docMax = max(bagOfWords[course].values())\n",
    "    for termIdx, term in termsOrder:\n",
    "        if(term not in bagOfWords[course]):\n",
    "            continue\n",
    "        # We use the double normalization 0.5 for the tf\n",
    "        tf[termIdx][courseIdx] = 0.5+0.5*bagOfWords[course][term]/docMax\n",
    "        overallFreq[termIdx] += 1\n",
    "overallFreq = np.log(numCourses/overallFreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hadoop\n",
      "recommender\n",
      "ecommerce\n",
      "auction\n",
      "ad\n",
      "realworld\n",
      "mapreduce\n",
      "advertisement\n",
      "selfcontained\n",
      "mining\n",
      "service\n",
      "seek\n",
      "foundational\n",
      "spark\n",
      "networking\n"
     ]
    }
   ],
   "source": [
    "tf_idf = tf*np.tile(overallFreq,(numCourses,1)).T\n",
    "np.save('X',tf_idf)\n",
    "for term in map(lambda x: idx2Term[x],tf_idf[:,course2Idx[name2id['Internet analytics']]].argsort()[-15:][::-1]):\n",
    "    print(term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.3: Document similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def docSimilarity(di,dj):\n",
    "    return np.dot(di,dj)/(np.sqrt(np.dot(di,di))*np.sqrt(np.dot(dj,dj)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Markov chains and algorithmic applications', 'Applied probability & stochastic processes', 'Applied stochastic processes', 'Internet analytics', 'Optimization and simulation']\n",
      "[[ 1.          0.20735236  0.1435479   0.13775226  0.13900297]\n",
      " [ 0.20735236  1.          0.13376437  0.07090623  0.15045302]\n",
      " [ 0.1435479   0.13376437  1.          0.05314457  0.10490284]\n",
      " [ 0.13775226  0.07090623  0.05314457  1.          0.09714256]\n",
      " [ 0.13900297  0.15045302  0.10490284  0.09714256  1.        ]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <div class=\"plotdiv\" id=\"aed58716-6dc0-4aeb-bdd2-081d8dcda838\"></div>\n",
       "    </div>\n",
       "<script type=\"text/javascript\">\n",
       "  \n",
       "  (function(global) {\n",
       "    function now() {\n",
       "      return new Date();\n",
       "    }\n",
       "  \n",
       "    var force = \"\";\n",
       "  \n",
       "    if (typeof (window._bokeh_onload_callbacks) === \"undefined\" || force !== \"\") {\n",
       "      window._bokeh_onload_callbacks = [];\n",
       "      window._bokeh_is_loading = undefined;\n",
       "    }\n",
       "  \n",
       "  \n",
       "    \n",
       "    if (typeof (window._bokeh_timeout) === \"undefined\" || force !== \"\") {\n",
       "      window._bokeh_timeout = Date.now() + 0;\n",
       "      window._bokeh_failed_load = false;\n",
       "    }\n",
       "  \n",
       "    var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "       \"<div style='background-color: #fdd'>\\n\"+\n",
       "       \"<p>\\n\"+\n",
       "       \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "       \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "       \"</p>\\n\"+\n",
       "       \"<ul>\\n\"+\n",
       "       \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "       \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "       \"</ul>\\n\"+\n",
       "       \"<code>\\n\"+\n",
       "       \"from bokeh.resources import INLINE\\n\"+\n",
       "       \"output_notebook(resources=INLINE)\\n\"+\n",
       "       \"</code>\\n\"+\n",
       "       \"</div>\"}};\n",
       "  \n",
       "    function display_loaded() {\n",
       "      if (window.Bokeh !== undefined) {\n",
       "        Bokeh.$(\"#aed58716-6dc0-4aeb-bdd2-081d8dcda838\").text(\"BokehJS successfully loaded.\");\n",
       "      } else if (Date.now() < window._bokeh_timeout) {\n",
       "        setTimeout(display_loaded, 100)\n",
       "      }\n",
       "    }\n",
       "  \n",
       "    function run_callbacks() {\n",
       "      window._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "      delete window._bokeh_onload_callbacks\n",
       "      console.info(\"Bokeh: all callbacks have finished\");\n",
       "    }\n",
       "  \n",
       "    function load_libs(js_urls, callback) {\n",
       "      window._bokeh_onload_callbacks.push(callback);\n",
       "      if (window._bokeh_is_loading > 0) {\n",
       "        console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "        return null;\n",
       "      }\n",
       "      if (js_urls == null || js_urls.length === 0) {\n",
       "        run_callbacks();\n",
       "        return null;\n",
       "      }\n",
       "      console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "      window._bokeh_is_loading = js_urls.length;\n",
       "      for (var i = 0; i < js_urls.length; i++) {\n",
       "        var url = js_urls[i];\n",
       "        var s = document.createElement('script');\n",
       "        s.src = url;\n",
       "        s.async = false;\n",
       "        s.onreadystatechange = s.onload = function() {\n",
       "          window._bokeh_is_loading--;\n",
       "          if (window._bokeh_is_loading === 0) {\n",
       "            console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "            run_callbacks()\n",
       "          }\n",
       "        };\n",
       "        s.onerror = function() {\n",
       "          console.warn(\"failed to load library \" + url);\n",
       "        };\n",
       "        console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      }\n",
       "    };var element = document.getElementById(\"aed58716-6dc0-4aeb-bdd2-081d8dcda838\");\n",
       "    if (element == null) {\n",
       "      console.log(\"Bokeh: ERROR: autoload.js configured with elementid 'aed58716-6dc0-4aeb-bdd2-081d8dcda838' but no matching script tag was found. \")\n",
       "      return false;\n",
       "    }\n",
       "  \n",
       "    var js_urls = [];\n",
       "  \n",
       "    var inline_js = [\n",
       "      function(Bokeh) {\n",
       "        Bokeh.$(function() {\n",
       "            var docs_json = {\"cbc8b38d-29bd-47df-bd2e-fc652ace2c8a\":{\"roots\":{\"references\":[{\"attributes\":{\"callback\":null,\"column_names\":[\"courses\",\"course0\",\"course1\",\"course2\",\"course3\",\"course4\"],\"data\":{\"course0\":[1.0,0.20735235647065148,0.14354789533441262,0.1377522576888123,0.13900296740128837],\"course1\":[0.20735235647065148,1.0000000000000002,0.1337643667675543,0.07090622831229136,0.15045302103041022],\"course2\":[0.14354789533441262,0.1337643667675543,1.0000000000000002,0.05314457219355201,0.10490283502666427],\"course3\":[0.1377522576888123,0.07090622831229136,0.05314457219355201,1.0,0.09714256193612432],\"course4\":[0.13900296740128837,0.15045302103041022,0.10490283502666427,0.09714256193612432,1.0],\"courses\":[\"Markov chains and algorithmic applications\",\"Applied probability & stochastic processes\",\"Applied stochastic processes\",\"Internet analytics\",\"Optimization and simulation\"]}},\"id\":\"5728ddaf-b357-49af-bed8-bcb160be324c\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"48a55176-934a-4c98-8096-d68759fe921d\",\"type\":\"StringEditor\"},{\"attributes\":{\"children\":[{\"id\":\"24d96fa0-77db-4688-aa8e-4040beb2a10c\",\"type\":\"DataTable\"}]},\"id\":\"95eea729-dfc3-41c8-bca1-7b5bb60aaecc\",\"type\":\"WidgetBox\"},{\"attributes\":{\"editor\":{\"id\":\"246b0048-3f60-489c-b6df-50deda50dd36\",\"type\":\"StringEditor\"},\"field\":\"courses\",\"formatter\":{\"id\":\"541ef8bb-ad66-4285-b002-5eb436b5721f\",\"type\":\"StringFormatter\"},\"title\":\"Similarity\"},\"id\":\"507fec2b-3575-4763-a3d2-ae0043cf5fd5\",\"type\":\"TableColumn\"},{\"attributes\":{\"columns\":[{\"id\":\"507fec2b-3575-4763-a3d2-ae0043cf5fd5\",\"type\":\"TableColumn\"},{\"id\":\"0961cc7d-fe65-4d79-bdba-394af4b0b3c4\",\"type\":\"TableColumn\"},{\"id\":\"6afe18fb-eb7b-48f6-8277-2167c934eed0\",\"type\":\"TableColumn\"},{\"id\":\"101148b5-2ffd-41d0-a204-c72e40b95dbd\",\"type\":\"TableColumn\"},{\"id\":\"6b8797ee-489c-4ee0-8e16-707271e736d3\",\"type\":\"TableColumn\"},{\"id\":\"ed83e237-6568-40f0-92f1-f1a762e3dc68\",\"type\":\"TableColumn\"}],\"source\":{\"id\":\"5728ddaf-b357-49af-bed8-bcb160be324c\",\"type\":\"ColumnDataSource\"}},\"id\":\"24d96fa0-77db-4688-aa8e-4040beb2a10c\",\"type\":\"DataTable\"},{\"attributes\":{\"editor\":{\"id\":\"48a55176-934a-4c98-8096-d68759fe921d\",\"type\":\"StringEditor\"},\"field\":\"course0\",\"formatter\":{\"id\":\"5491cb68-1933-463c-ab52-7a5f767e4cae\",\"type\":\"StringFormatter\"},\"title\":\"Markov chains and algorithmic applications\"},\"id\":\"0961cc7d-fe65-4d79-bdba-394af4b0b3c4\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"8131661a-6646-42e0-876e-b561b4aa6d8f\",\"type\":\"StringEditor\"},{\"attributes\":{\"editor\":{\"id\":\"aa98eacb-ffff-43fa-ab6d-493a3c3d813a\",\"type\":\"StringEditor\"},\"field\":\"course4\",\"formatter\":{\"id\":\"7c9e9d65-a21b-4405-8c9a-6de7e63e141e\",\"type\":\"StringFormatter\"},\"title\":\"Optimization and simulation\"},\"id\":\"ed83e237-6568-40f0-92f1-f1a762e3dc68\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"510a34a9-1816-400c-a173-c696adda7f7e\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"541ef8bb-ad66-4285-b002-5eb436b5721f\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"123768f7-7dd0-4228-a51f-77ec1bac2c98\",\"type\":\"StringFormatter\"},{\"attributes\":{\"editor\":{\"id\":\"8131661a-6646-42e0-876e-b561b4aa6d8f\",\"type\":\"StringEditor\"},\"field\":\"course3\",\"formatter\":{\"id\":\"90b77a75-0c9a-4d8c-b0c7-a74f69821f29\",\"type\":\"StringFormatter\"},\"title\":\"Internet analytics\"},\"id\":\"6b8797ee-489c-4ee0-8e16-707271e736d3\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"ebf06fe5-13f2-46c7-8bbf-c969a76e2ad7\",\"type\":\"StringFormatter\"},{\"attributes\":{\"editor\":{\"id\":\"7f547066-151e-4222-8341-8836934596ad\",\"type\":\"StringEditor\"},\"field\":\"course2\",\"formatter\":{\"id\":\"123768f7-7dd0-4228-a51f-77ec1bac2c98\",\"type\":\"StringFormatter\"},\"title\":\"Applied stochastic processes\"},\"id\":\"101148b5-2ffd-41d0-a204-c72e40b95dbd\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"aa98eacb-ffff-43fa-ab6d-493a3c3d813a\",\"type\":\"StringEditor\"},{\"attributes\":{\"editor\":{\"id\":\"510a34a9-1816-400c-a173-c696adda7f7e\",\"type\":\"StringEditor\"},\"field\":\"course1\",\"formatter\":{\"id\":\"ebf06fe5-13f2-46c7-8bbf-c969a76e2ad7\",\"type\":\"StringFormatter\"},\"title\":\"Applied probability & stochastic processes\"},\"id\":\"6afe18fb-eb7b-48f6-8277-2167c934eed0\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"246b0048-3f60-489c-b6df-50deda50dd36\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"7c9e9d65-a21b-4405-8c9a-6de7e63e141e\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"90b77a75-0c9a-4d8c-b0c7-a74f69821f29\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"7f547066-151e-4222-8341-8836934596ad\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"5491cb68-1933-463c-ab52-7a5f767e4cae\",\"type\":\"StringFormatter\"}],\"root_ids\":[\"95eea729-dfc3-41c8-bca1-7b5bb60aaecc\"]},\"title\":\"Bokeh Application\",\"version\":\"0.12.2\"}};\n",
       "            var render_items = [{\"docid\":\"cbc8b38d-29bd-47df-bd2e-fc652ace2c8a\",\"elementid\":\"aed58716-6dc0-4aeb-bdd2-081d8dcda838\",\"modelid\":\"95eea729-dfc3-41c8-bca1-7b5bb60aaecc\"}];\n",
       "            \n",
       "            Bokeh.embed.embed_items(docs_json, render_items);\n",
       "        });\n",
       "      },\n",
       "      function(Bokeh) {\n",
       "      }\n",
       "    ];\n",
       "  \n",
       "    function run_inline_js() {\n",
       "      \n",
       "      if ((window.Bokeh !== undefined) || (force === \"1\")) {\n",
       "        for (var i = 0; i < inline_js.length; i++) {\n",
       "          inline_js[i](window.Bokeh);\n",
       "        }if (force === \"1\") {\n",
       "          display_loaded();\n",
       "        }} else if (Date.now() < window._bokeh_timeout) {\n",
       "        setTimeout(run_inline_js, 100);\n",
       "      } else if (!window._bokeh_failed_load) {\n",
       "        console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "        window._bokeh_failed_load = true;\n",
       "      } else if (!force) {\n",
       "        var cell = $(\"#aed58716-6dc0-4aeb-bdd2-081d8dcda838\").parents('.cell').data().cell;\n",
       "        cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "      }\n",
       "  \n",
       "    }\n",
       "  \n",
       "    if (window._bokeh_is_loading === 0) {\n",
       "      console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "      run_inline_js();\n",
       "    } else {\n",
       "      load_libs(js_urls, function() {\n",
       "        console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "        run_inline_js();\n",
       "      });\n",
       "    }\n",
       "  }(this));\n",
       "</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "top = (tf_idf[term2Idx['facebook']]+tf_idf[term2Idx['markov']]+tf_idf[word2Idx['chain']]).argsort()[-5:][::-1]\n",
    "topCol = list(map(lambda x: tf_idf[:,x],top))\n",
    "topCourses = list(map(lambda x: id2name[idx2Course[x]],top))\n",
    "cmp = np.zeros((5,5))\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        cmp[i][j] = docSimilarity(topCol[i],topCol[j])\n",
    "print(topCourses)\n",
    "print(cmp)\n",
    "\n",
    "data = dict(\n",
    "    courses=topCourses,\n",
    "    course0=cmp[0],\n",
    "    course1=cmp[1],\n",
    "    course2=cmp[2],\n",
    "    course3=cmp[3],\n",
    "    course4=cmp[4]\n",
    ")\n",
    "source = ColumnDataSource(data)\n",
    "columns = [TableColumn(field='courses', title='Similarity')]\n",
    "columns = columns + list(map(lambda x: TableColumn(field='course'+str(x[0]), title=x[1]),enumerate(topCourses)))\n",
    "data_table = DataTable(source=source,columns=columns)\n",
    "show(widgetbox(data_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
