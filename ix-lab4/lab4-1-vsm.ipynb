{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text 1: Vector space models\n",
    "**Internet Analytics - Lab 4**\n",
    "\n",
    "---\n",
    "\n",
    "**Group:** R\n",
    "**Names:**\n",
    "\n",
    "* Raphael Strebel\n",
    "* Raphaël Barman\n",
    "* Thierry Bossy\n",
    "\n",
    "---\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "*This is a template for part 1 of the lab. Clearly write your answers, comments and interpretations in Markodown cells. Don't forget that you can add $\\LaTeX$ equations in these cells. Feel free to add or remove any cell.*\n",
    "\n",
    "*Please properly comment your code. Code readability will be considered for grading. To avoid long cells of codes in the notebook, you can also embed long python functions and classes in a separate module. Don’t forget to hand in your module if that is the case. In multiple exercises, you are required to come up with your own method to solve various problems. Be creative and clearly motivate and explain your methods. Creativity and clarity will be considered for grading.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import csc_matrix\n",
    "from utils import load_json, load_pkl\n",
    "import string\n",
    "import re\n",
    "from operator import itemgetter\n",
    "import nltk\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from nltk import word_tokenize\n",
    "\n",
    "courses = load_json('data/courses.txt')\n",
    "stopwords = load_pkl('data/stopwords.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.1: Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id2name = dict(map(itemgetter('courseId', 'name'),courses))\n",
    "name2id = {v: k for k,v in id2name.items()}\n",
    "np.save('id2name', id2name)\n",
    "np.save('name2id', name2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lmtzr = WordNetLemmatizer()\n",
    "\n",
    "def isValidWord(word):\n",
    "    return not word.lower() in stopwords and not word.isdigit() and not word in string.punctuation and not '/' in word\n",
    "\n",
    "def processWord(word):\n",
    "    return lmtzr.lemmatize(word.lower())\n",
    "\n",
    "# Returns the bag of words of a text as a dictionary, so the different words as keys and their number of occurence as value\n",
    "def getCourseBagOfWords(text):\n",
    "    # We create a default dict which returns 0 when an item is not in\n",
    "    bow = defaultdict(lambda: 0)\n",
    "    \n",
    "    punctuation = string.punctuation.replace('-','')\n",
    "    text = text.translate(str.maketrans('','',punctuation))\n",
    "    \n",
    "    text = nltk.word_tokenize(text)\n",
    "\n",
    "    for idx in range(len(text)):\n",
    "        word = text[idx]\n",
    "        # separate words such that \"MyNameIsChristian\"\n",
    "        # becomes \"My\" \"Name\" \"Is\" \"Christian\"\n",
    "        # however makes sure that word such as \"USA\" and \"imageJ\" are untouched\n",
    "        res = re.findall('[a-zA-Z][^A-Z]*',word)\n",
    "        # We check that we have a match of several words\n",
    "        if len(res) > 1:\n",
    "            # Check if than we dont have a one letter word\n",
    "            if len(min(res,key=len)) > 1:\n",
    "                # delete initial world\n",
    "                del text[idx]\n",
    "                # Add the new words\n",
    "                for offset, match in enumerate(res):\n",
    "                    text.insert(idx+offset,match)\n",
    "                            \n",
    "    for word in text:\n",
    "        if isValidWord(word):\n",
    "            bow[processWord(word)] += 1\n",
    "\n",
    "    return bow\n",
    "\n",
    "# Computes the bag of word for each course\n",
    "# and the global bag of words\n",
    "def getBagOfWords():\n",
    "    globalBagOfWords = defaultdict(lambda: 0)\n",
    "    bagOfWords = {}\n",
    "    for course in courses:\n",
    "        bow = getCourseBagOfWords(course['description'])\n",
    "        bagOfWords[course['courseId']] = bow\n",
    "        for k,v in bow.items():\n",
    "            globalBagOfWords[k] += v\n",
    "    \n",
    "    occurences = sorted(globalBagOfWords.items(), key=itemgetter(1))\n",
    "    # We remove all words with occurences < minBound and > maxBound\n",
    "    # where minBound is the occurence of the lowest 20th term\n",
    "    # and maxBound the occurence of the highest 20th term\n",
    "    minBound = occurences[20][1]\n",
    "    maxBound = occurences[-20][1]\n",
    "    globalBagOfWords = {k: v for k,v in globalBagOfWords.items() if v > minBound and v < maxBound}\n",
    "    # Numbers manually found by looking at the occurences\n",
    "    for course in bagOfWords.keys():\n",
    "        bagOfWords[course] = {k: v for k,v in bagOfWords[course].items() if k in globalBagOfWords}\n",
    "    return globalBagOfWords, bagOfWords\n",
    "globalBagOfWords, bagOfWords = getBagOfWords()\n",
    "np.save('bagOfWords', bagOfWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Explain which ones you implemented and why.\n",
    "We chose to remove all punctuation and all stopwords since there really is no interest in keeping them.\n",
    "\n",
    "We also lemmatize the words using the nltk library, to keep track of similar words and have a more accurate word occurence count.\n",
    "\n",
    "We remove every words that was as present as the 20 most/least present words.\n",
    "\n",
    "## 2.Print the terms in the pre-processed description of the IX class in alphabetical order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words for Internet analytics course:\n",
      "   - acquired\n",
      "   - activity\n",
      "   - ad\n",
      "   - advertisement\n",
      "   - algebra\n",
      "   - algorithm\n",
      "   - analytics\n",
      "   - analyze\n",
      "   - application\n",
      "   - auction\n",
      "   - balance\n",
      "   - based\n",
      "   - cathedra\n",
      "   - chain\n",
      "   - class\n",
      "   - cloud\n",
      "   - clustering\n",
      "   - collection\n",
      "   - com-300\n",
      "   - combination\n",
      "   - communication\n",
      "   - community\n",
      "   - computing\n",
      "   - concrete\n",
      "   - coverage\n",
      "   - current\n",
      "   - datasets\n",
      "   - decade\n",
      "   - dedicated\n",
      "   - designed\n",
      "   - detection\n",
      "   - develop\n",
      "   - dimensionality\n",
      "   - draw\n",
      "   - e-commerce\n",
      "   - effectiveness\n",
      "   - efficiency\n",
      "   - exam\n",
      "   - expected\n",
      "   - explore\n",
      "   - explores\n",
      "   - field\n",
      "   - final\n",
      "   - foundational\n",
      "   - framework\n",
      "   - function\n",
      "   - fundamental\n",
      "   - good\n",
      "   - graph\n",
      "   - hadoop\n",
      "   - hands-on\n",
      "   - homework\n",
      "   - important\n",
      "   - information\n",
      "   - infrastructure\n",
      "   - inspired\n",
      "   - internet\n",
      "   - java\n",
      "   - key\n",
      "   - keywords\n",
      "   - knowledge\n",
      "   - lab\n",
      "   - laboratory\n",
      "   - large-scale\n",
      "   - linear\n",
      "   - machine\n",
      "   - main\n",
      "   - map-reduce\n",
      "   - markov\n",
      "   - material\n",
      "   - medium\n",
      "   - midterm\n",
      "   - mining\n",
      "   - modeling\n",
      "   - network\n",
      "   - networking\n",
      "   - number\n",
      "   - online\n",
      "   - past\n",
      "   - practical\n",
      "   - practice\n",
      "   - problem\n",
      "   - provide\n",
      "   - question\n",
      "   - real\n",
      "   - real-world\n",
      "   - recommended\n",
      "   - recommender\n",
      "   - reduction\n",
      "   - related\n",
      "   - required\n",
      "   - retrieval\n",
      "   - search\n",
      "   - seek\n",
      "   - self-contained\n",
      "   - service\n",
      "   - session\n",
      "   - social\n",
      "   - spark\n",
      "   - specifically\n",
      "   - start\n",
      "   - statistic\n",
      "   - stochastic\n",
      "   - stream\n",
      "   - structure\n",
      "   - technique\n",
      "   - theoretical\n",
      "   - theory\n",
      "   - topic\n",
      "   - typical\n",
      "   - ubiquitous\n",
      "   - user\n",
      "   - weekly\n",
      "   - work\n",
      "   - world\n"
     ]
    }
   ],
   "source": [
    "ixBow = bagOfWords[name2id['Internet analytics']]\n",
    "print('Words for Internet analytics course:')\n",
    "for words in sorted(ixBow.keys(),key=lambda v: v.upper()):\n",
    "    print('   -',words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.2: Term-document matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms for IX:\n",
      "   - service \t3.614\n",
      "   - online \t3.454\n",
      "   - real-world \t3.412\n",
      "   - social \t3.195\n",
      "   - mining \t2.802\n",
      "   - explore \t2.679\n",
      "   - networking \t2.611\n",
      "   - large-scale \t2.425\n",
      "   - hadoop \t2.423\n",
      "   - e-commerce \t2.423\n",
      "   - recommender \t2.261\n",
      "   - auction \t2.145\n",
      "   - internet \t1.983\n",
      "   - datasets \t1.922\n",
      "   - ad \t1.821\n"
     ]
    }
   ],
   "source": [
    "# Some helpers variable\n",
    "numTerms = len(globalBagOfWords.keys())\n",
    "numCourses = len(bagOfWords.keys())\n",
    "termsOrder = list(enumerate(globalBagOfWords.keys()))\n",
    "coursesOrder = list(enumerate(bagOfWords.keys()))\n",
    "idx2Term = {i[0]: i[1] for i in termsOrder}\n",
    "term2Idx = {v: k for k,v in idx2Term.items()}\n",
    "idx2Course = {i[0]: i[1] for i in coursesOrder}\n",
    "course2Idx = {v: k for k,v in idx2Course.items()}\n",
    "\n",
    "np.save('idx2Term', idx2Term)\n",
    "np.save('term2Idx', term2Idx)\n",
    "np.save('idx2Course', idx2Course)\n",
    "np.save('course2Idx', course2Idx)\n",
    "\n",
    "overallFreq = np.zeros(numTerms)\n",
    "row = []\n",
    "col = []\n",
    "data  = []\n",
    "\n",
    "# We construct the term document matrix\n",
    "for courseIdx, course in coursesOrder:\n",
    "    if(len(bagOfWords[course]) == 0):\n",
    "        continue\n",
    "    docMax = max(bagOfWords[course].values())\n",
    "    for termIdx, term in termsOrder:\n",
    "        if(term not in bagOfWords[course]):\n",
    "            continue\n",
    "        \n",
    "        row.append(termIdx)\n",
    "        col.append(courseIdx)\n",
    "        # We use the formual seen in class\n",
    "        data.append(bagOfWords[course][term]/docMax)\n",
    "        # We construct at the same time what we need for the idf\n",
    "        overallFreq[termIdx] += 1\n",
    "\n",
    "# We construct the sparse matrix\n",
    "tf_idf = csc_matrix((data,(row,col)))\n",
    "\n",
    "# We compute the idf\n",
    "overallFreq = np.log(numCourses/overallFreq)\n",
    "\n",
    "np.save('idf',overallFreq)\n",
    "\n",
    "# tf_idf = tf.copy()\n",
    "# We multiply each row by the corresponding idf, hence the need for the initial csr\n",
    "# tf_idf.data *= overallFreq.repeat(np.diff(tf_idf.indptr))\n",
    "tf_idf.data *= overallFreq[tf_idf.indices]\n",
    "\n",
    "np.save('X',tf_idf)\n",
    "\n",
    "# We transform to csc since we will only make column queries\n",
    "# tf_idf = tf_idf.tocsc()\n",
    "\n",
    "\n",
    "ix = tf_idf.getcol(course2Idx[name2id['Internet analytics']])\n",
    "\n",
    "topTerms = np.argsort(-ix.data)[:15]\n",
    "print('Top terms for IX:')\n",
    "for term in topTerms:\n",
    "    print('   -',idx2Term[ix.indices[term]],'\\t%.3f'%ix.data[term])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between high scores and low scores can be explained by the fact that some terms appears a lot in this course's description and not in the other ones. I.e. service is certainly a term that is used a lot in this course, and not in the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.3: Document similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top courses with query \"markov chain\":\n",
      "   - Applied probability & stochastic processes \t0.566\n",
      "   - Applied stochastic processes \t0.527\n",
      "   - Markov chains and algorithmic applications \t0.363\n",
      "   - Supply chain management \t0.349\n",
      "   - Statistical Sequence Processing \t0.302\n",
      "Top courses with query \"facebook\":\n",
      "   - Computational Social Media \t0.187\n"
     ]
    }
   ],
   "source": [
    "def docSim(di,dj):\n",
    "    # Norm is done using underlying data vector, much faster than using transpose\n",
    "    return (di.dot(dj)/(np.sqrt((di.data ** 2).sum())*np.sqrt((dj.data ** 2).sum()))).A[0][0]\n",
    "\n",
    "# Gives the top 5 course for given terms\n",
    "# Takes a space separated list of terms as argument\n",
    "def query(terms):\n",
    "    terms = terms.split(' ')\n",
    "    \n",
    "    # We get the tf-idf for the terms, tf is always 1 and idf was already computed\n",
    "    data = [overallFreq[term2Idx[term]] for term in terms]\n",
    "    cols = [term2Idx[term] for term in terms]\n",
    "    rows = [0] * len(terms)\n",
    "    \n",
    "    # We create the query of shape 1 x M\n",
    "    query = csc_matrix((data,(rows,cols)),shape=(1,numTerms))\n",
    "    \n",
    "    # We run document similarity with the query for all the documents\n",
    "    results = np.array([docSim(query,tf_idf.getcol(doc)) for doc in range(numCourses)])\n",
    "    top = np.argsort(-results)[:5]\n",
    "    # We filter all courses with null scores\n",
    "    top = [course for course in top if results[course] != 0.0]\n",
    "    \n",
    "    print('Top courses with query \"'+\" \".join(terms)+'\":')\n",
    "    for course in top:\n",
    "        print('   -',id2name[idx2Course[course]],'\\t%.3f'%results[course])\n",
    "query('markov chain')\n",
    "query('facebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 'markov chains', since it is a two term query, we have to understand the influence of each term. From what we see, the term 'markov' certainly has an higher idf than 'chain', since 'chain' is a very current word and can be associated with other words or subject than 'markov' whereas 'markov' is most often associated with 'chain'. This explain that the top results are about stochastic processes, which is among other things about markov chains.\n",
    "\n",
    "Note that the course about markov chain certainly doesn't repeat too much the term 'markov chain' since it is in the course name.\n",
    "\n",
    "Supply chain management doesn't talk about markov chain, however the tf of 'chain' in its description must be very high, since it compensates for the absence of the term 'markov'.\n",
    "\n",
    "For the 'facebook' query, there is only one course which has a non null score. This is simply due to the fact that it is the only course which contains the 'facebook' term."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
