{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text 1: Vector space models\n",
    "**Internet Analytics - Lab 4**\n",
    "\n",
    "---\n",
    "\n",
    "**Group:** R\n",
    "**Names:**\n",
    "\n",
    "* Raphael Strebel\n",
    "* Raphaël Barman\n",
    "* Thierry Bossy\n",
    "\n",
    "---\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "*This is a template for part 1 of the lab. Clearly write your answers, comments and interpretations in Markodown cells. Don't forget that you can add $\\LaTeX$ equations in these cells. Feel free to add or remove any cell.*\n",
    "\n",
    "*Please properly comment your code. Code readability will be considered for grading. To avoid long cells of codes in the notebook, you can also embed long python functions and classes in a separate module. Don’t forget to hand in your module if that is the case. In multiple exercises, you are required to come up with your own method to solve various problems. Be creative and clearly motivate and explain your methods. Creativity and clarity will be considered for grading.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"http://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"0ecddde7-f18e-432b-8fd1-730ba81e7fa5\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(global) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = \"1\";\n",
       "\n",
       "  if (typeof (window._bokeh_onload_callbacks) === \"undefined\" || force !== \"\") {\n",
       "    window._bokeh_onload_callbacks = [];\n",
       "    window._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "\n",
       "  \n",
       "  if (typeof (window._bokeh_timeout) === \"undefined\" || force !== \"\") {\n",
       "    window._bokeh_timeout = Date.now() + 5000;\n",
       "    window._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    if (window.Bokeh !== undefined) {\n",
       "      Bokeh.$(\"#0ecddde7-f18e-432b-8fd1-730ba81e7fa5\").text(\"BokehJS successfully loaded.\");\n",
       "    } else if (Date.now() < window._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    window._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    delete window._bokeh_onload_callbacks\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    window._bokeh_onload_callbacks.push(callback);\n",
       "    if (window._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    window._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        window._bokeh_is_loading--;\n",
       "        if (window._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"0ecddde7-f18e-432b-8fd1-730ba81e7fa5\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '0ecddde7-f18e-432b-8fd1-730ba81e7fa5' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = ['https://cdn.pydata.org/bokeh/release/bokeh-0.12.2.min.js', 'https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.2.min.js', 'https://cdn.pydata.org/bokeh/release/bokeh-compiler-0.12.2.min.js'];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "      Bokeh.$(\"#0ecddde7-f18e-432b-8fd1-730ba81e7fa5\").text(\"BokehJS is loading...\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.2.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.2.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.2.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.2.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((window.Bokeh !== undefined) || (force === \"1\")) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i](window.Bokeh);\n",
       "      }if (force === \"1\") {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < window._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!window._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      window._bokeh_failed_load = true;\n",
       "    } else if (!force) {\n",
       "      var cell = $(\"#0ecddde7-f18e-432b-8fd1-730ba81e7fa5\").parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (window._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(this));"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from utils import load_json, load_pkl\n",
    "import string\n",
    "import re\n",
    "from operator import itemgetter\n",
    "import nltk\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "from bokeh.plotting import figure, output_notebook,show, ColumnDataSource\n",
    "from bokeh.models.widgets import DataTable, DateFormatter, TableColumn\n",
    "from bokeh.layouts import widgetbox\n",
    "\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "courses = load_json('data/courses.txt')\n",
    "stopwords = load_pkl('data/stopwords.pkl')\n",
    "stopwords.add('-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.1: Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id2name = dict(map(itemgetter('courseId', 'name'),courses))\n",
    "name2id = {v: k for k,v in id2name.items()}\n",
    "np.save('id2name', id2name)\n",
    "np.save('name2id', name2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lmtzr = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Returns the bag of words of a text as a dictionary, so the different words as keys and their number of occurence as value\n",
    "def getCourseBagOfWords(text):\n",
    "    # We create a default dict which returns 0 when an item is not in\n",
    "    bow = defaultdict(lambda: 0)\n",
    "    \n",
    "    # Change inceasable space to normal space...\n",
    "    text = text.replace('\\xa0', ' ')\n",
    "    # Remove punctuation, don't remove '-'\n",
    "    punctuation = string.punctuation.replace('-','')\n",
    "    text = text.translate(str.maketrans('','',punctuation))\n",
    "    text = text.split(' ')\n",
    "    for idx in range(len(text)):\n",
    "        word = text[idx]\n",
    "        # separate words such that \"MyNameIsChristian\"\n",
    "        # becomes \"My\" \"Name\" \"Is\" \"Christian\"\n",
    "        # however makes sure that word such as \"USA\" and \"imageJ\" are untouched\n",
    "        res = re.findall('[a-zA-Z][^A-Z]*',word)\n",
    "        # We check that we have a match of several words\n",
    "        if len(res) > 1:\n",
    "            # Check if than we dont have a one letter word\n",
    "            if len(min(res,key=len)) != 1:\n",
    "                # delete initial world\n",
    "                del text[idx]\n",
    "                # Add the new words\n",
    "                for offset, match in enumerate(res):\n",
    "                    text.insert(idx+offset,match)\n",
    "    for word in text:\n",
    "        # Only lemmatize and stem words that are not uppercase\n",
    "        # (we don't want IT to become it)\n",
    "        if not word.isupper():\n",
    "            word = word.lower()\n",
    "            word = lmtzr.lemmatize(word)\n",
    "         # Skip stopwords and digits\n",
    "        if not word in stopwords and not word.isdigit():\n",
    "            bow[word] += 1\n",
    "    return bow\n",
    "\n",
    "# Computes the bag of word for each course\n",
    "# and the global bag of words\n",
    "def getBagOfWords():\n",
    "    globalBagOfWords = defaultdict(lambda: 0)\n",
    "    bagOfWords = {}\n",
    "    for course in courses:\n",
    "        bow = getCourseBagOfWords(course['description'])\n",
    "        bagOfWords[course['courseId']] = bow\n",
    "        for k,v in bow.items():\n",
    "            globalBagOfWords[k] += v\n",
    "    \n",
    "    occurences = sorted(globalBagOfWords.items(), key=itemgetter(1))\n",
    "    # We remove all words with occurences < minBound and > maxBound\n",
    "    # where minBound is the occurence of the lowest 10th term\n",
    "    # and maxBound the occurence of the highest 10th term\n",
    "    minBound = occurences[9][1]\n",
    "    maxBound = occurences[-9][1]\n",
    "    globalBagOfWords = {k: v for k,v in globalBagOfWords.items() if v > minBound and v < maxBound}\n",
    "    for course in bagOfWords.keys():\n",
    "        bagOfWords[course] = {k: v for k,v in bagOfWords[course].items() if k in globalBagOfWords}\n",
    "    return globalBagOfWords, bagOfWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123728\n",
      "7022\n"
     ]
    }
   ],
   "source": [
    "globalBagOfWords, bagOfWords = getBagOfWords()\n",
    "print(sum(globalBagOfWords.values()))\n",
    "print(len(globalBagOfWords.keys()))\n",
    "#getCourseBagOfWords(courses[1]['description'])\n",
    "#for course in courses:\n",
    "#    bow = getBagOfWords(course['description'])\n",
    "#    bagOfWords[course['courseId']] = bow\n",
    "#    mergeBow(globalBagOfWord,bow)\n",
    "#test_course = course.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Explain which ones you implemented and why.\n",
    "We chose to remove all punctuation and all stopwords since there really is no interest in keeping them.\n",
    "\n",
    "We also lemmatize the words using the nltk library, to keep track of similar words and have a more accurate word occurence count.\n",
    "\n",
    "## 2.Print the terms in the pre-processed description of the IX class in alphabetical order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words for Internet analytics course:\n",
      "   - acquired\n",
      "   - activity\n",
      "   - ad\n",
      "   - advertisement\n",
      "   - algebra\n",
      "   - algorithm\n",
      "   - analysis\n",
      "   - analytics\n",
      "   - analyze\n",
      "   - apache\n",
      "   - application\n",
      "   - assessment\n",
      "   - auction\n",
      "   - balance\n",
      "   - based\n",
      "   - basic\n",
      "   - cathedra\n",
      "   - chain\n",
      "   - class\n",
      "   - cloud\n",
      "   - clustering\n",
      "   - collection\n",
      "   - COM-300\n",
      "   - combination\n",
      "   - communication\n",
      "   - community\n",
      "   - computing\n",
      "   - concept\n",
      "   - concrete\n",
      "   - content\n",
      "   - coverage\n",
      "   - curated\n",
      "   - current\n",
      "   - data\n",
      "   - datasets\n",
      "   - decade\n",
      "   - dedicated\n",
      "   - designed\n",
      "   - detection\n",
      "   - develop\n",
      "   - dimensionality\n",
      "   - draw\n",
      "   - e-commerce\n",
      "   - effectiveness\n",
      "   - efficiency\n",
      "   - end\n",
      "   - exam\n",
      "   - expected\n",
      "   - explore\n",
      "   - explores\n",
      "   - field\n",
      "   - final\n",
      "   - foundational\n",
      "   - framework\n",
      "   - function\n",
      "   - fundamental\n",
      "   - good\n",
      "   - graph\n",
      "   - hadoop\n",
      "   - hands-on\n",
      "   - homework\n",
      "   - important\n",
      "   - information\n",
      "   - infrastructure\n",
      "   - inspired\n",
      "   - internet\n",
      "   - java\n",
      "   - key\n",
      "   - keywords\n",
      "   - knowledge\n",
      "   - lab\n",
      "   - laboratory\n",
      "   - large-scale\n",
      "   - learning\n",
      "   - lecture\n",
      "   - linear\n",
      "   - machine\n",
      "   - main\n",
      "   - map-reduce\n",
      "   - markov\n",
      "   - material\n",
      "   - medium\n",
      "   - method\n",
      "   - midterm\n",
      "   - mining\n",
      "   - model\n",
      "   - modeling\n",
      "   - modelsdata-mining\n",
      "   - network\n",
      "   - networking\n",
      "   - number\n",
      "   - online\n",
      "   - outcome\n",
      "   - past\n",
      "   - practical\n",
      "   - practice\n",
      "   - prerequisite\n",
      "   - problem\n",
      "   - project\n",
      "   - provide\n",
      "   - question\n",
      "   - real\n",
      "   - real-world\n",
      "   - recommended\n",
      "   - recommender\n",
      "   - reduction\n",
      "   - related\n",
      "   - required\n",
      "   - retrieval\n",
      "   - search\n",
      "   - searchretrievaltopic\n",
      "   - seek\n",
      "   - self-contained\n",
      "   - service\n",
      "   - session\n",
      "   - social\n",
      "   - spark\n",
      "   - specifically\n",
      "   - start\n",
      "   - statistic\n",
      "   - stochastic\n",
      "   - stream\n",
      "   - structure\n",
      "   - student\n",
      "   - system\n",
      "   - teaching\n",
      "   - technique\n",
      "   - theoretical\n",
      "   - theory\n",
      "   - topic\n",
      "   - typical\n",
      "   - ubiquitous\n",
      "   - user\n",
      "   - weekly\n",
      "   - work\n",
      "   - world\n"
     ]
    }
   ],
   "source": [
    "ixBow = getCourseBagOfWords([course for course in courses if course['name'] == 'Internet analytics'][0]['description'])\n",
    "print('Words for Internet analytics course:')\n",
    "for words in sorted(ixBow.keys(),key=lambda v: v.upper()):\n",
    "    print('   -',words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.2: Term-document matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "globalBagOfWords, bagOfWords = getBagOfWords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numTerms = len(globalBagOfWords.keys())\n",
    "numCourses = len(bagOfWords.keys())\n",
    "termsOrder = list(enumerate(globalBagOfWords.keys()))\n",
    "coursesOrder = list(enumerate(bagOfWords.keys()))\n",
    "idx2Term = {i[0]: i[1] for i in termsOrder}\n",
    "term2Idx = {v: k for k,v in idx2Term.items()}\n",
    "idx2Course = {i[0]: i[1] for i in coursesOrder}\n",
    "course2Idx = {v: k for k,v in idx2Course.items()}\n",
    "\n",
    "np.save('idx2Term', idx2Term)\n",
    "np.save('term2Idx', term2Idx)\n",
    "np.save('idx2Course', idx2Course)\n",
    "np.save('course2Idx', course2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "overallFreq = np.zeros(numTerms)\n",
    "row = []\n",
    "col = []\n",
    "data  = []\n",
    "\n",
    "# We construct the term document matrix\n",
    "for courseIdx, course in coursesOrder:\n",
    "    if(len(bagOfWords[course]) == 0):\n",
    "        continue\n",
    "    docMax = max(bagOfWords[course].values())\n",
    "    for termIdx, term in termsOrder:\n",
    "        if(term not in bagOfWords[course]):\n",
    "            continue\n",
    "        col.append(courseIdx)\n",
    "        row.append(termIdx)\n",
    "        # We use the double normalization 0.5 for the tf\n",
    "        data.append(0.5+0.5*bagOfWords[course][term]/docMax)\n",
    "        # We construct at the same time what we need for the idf\n",
    "        overallFreq[termIdx] += 1\n",
    "\n",
    "# We construct the sparse matrix\n",
    "tf = csr_matrix((data,(row,col)))\n",
    "# We compute the idf\n",
    "overallFreq = np.log(numCourses/overallFreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e-commerce \t 4.03785600882\n",
      "hadoop \t 4.03785600882\n",
      "recommender \t 3.76754593675\n",
      "ad \t 3.57575788845\n",
      "auction \t 3.57575788845\n",
      "real-world \t 3.55418712\n",
      "self-contained \t 3.53312400772\n",
      "advertisement \t 3.53312400772\n",
      "map-reduce \t 3.53312400772\n",
      "mining \t 3.50286723908\n",
      "service \t 3.31323389637\n",
      "internet \t 3.30544781637\n",
      "foundational \t 3.29660269465\n",
      "seek \t 3.29660269465\n",
      "spark \t 3.29660269465\n"
     ]
    }
   ],
   "source": [
    "tf_idf = tf.copy()\n",
    "# We multiply each row by the corresponding idf\n",
    "tf_idf.data *= overallFreq.repeat(np.diff(tf_idf.indptr))\n",
    "\n",
    "np.save('X',tf_idf)\n",
    "# We convert the Compressed Sparse Row matrix to Compressed Sparse Column matrix\n",
    "# Since we need a particular course\n",
    "csc = tf_idf.tocsc()\n",
    "# Get the index of the course\n",
    "idx = course2Idx[name2id['Internet analytics']]\n",
    "# Get the offsets\n",
    "offset = csc.indptr[idx]\n",
    "offsetEnd = csc.indptr[idx+1]\n",
    "\n",
    "# We find the top 15 values' indices in the data and find the term corresponding to those\n",
    "topTerms = [(idx2Term[csc.indices[offset+index]],A.data[offset:offsetEnd][index])  for index in np.argsort(-A.data[offset:offsetEnd])[:15]]\n",
    "for term, score in topTerms:\n",
    "    print(term,'\\t',score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.3: Document similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def docSimilarity(di,dj):\n",
    "    return di.T.dot(dj)/(np.sqrt(di.T.dot(di))*np.sqrt(dj.T.dot(dj)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gives the top 5 course for given terms\n",
    "# Takes a space separated list of terms as argument\n",
    "def query(terms):\n",
    "    terms = terms.split(' ')\n",
    "    # Initialize with the first term\n",
    "    row = tf_idf.getrow(term2Idx[lmtzr.lemmatize(terms[0])])\n",
    "    # If there is more, add them\n",
    "    for term in terms[1:]:\n",
    "        row += tf_idf.getrow(term2Idx[lmtzr.lemmatize(term)])\n",
    "    # top in the \"local\" (csr) row\n",
    "    topLocal = np.argsort(-row.data)[:5]\n",
    "    #top in term of courses index\n",
    "    top = row.indices[topLocal]\n",
    "    topCourses = list(map(lambda x: id2name[idx2Course[x]],top))\n",
    "    print('Top courses with query \"'+\" \".join(terms)+'\":')\n",
    "    for idx,course in zip(topLocal,topCourses):\n",
    "        print('   -',course,'\\t%.3f'%row.data[idx])\n",
    "    numRes = len(top)\n",
    "    cmp = np.zeros((numRes,numRes))\n",
    "    for i in range(numRes):\n",
    "        for j in range(numRes):\n",
    "            cmp[i][j] = docSimilarity(tf_idf.getcol(top[i]),tf_idf.getcol(top[j]))\n",
    "    print('Comparison matrix:')\n",
    "    print(cmp)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top courses with query \"markov chains\":\n",
      "   - Applied probability & stochastic processes \t6.342\n",
      "   - Markov chains and algorithmic applications \t5.990\n",
      "   - Applied stochastic processes \t5.159\n",
      "   - Internet analytics \t4.193\n",
      "   - Stochastic calculus I \t4.193\n",
      "Comparison matrix:\n",
      "[[ 1.          0.21214307  0.13333225  0.07074898  0.21040076]\n",
      " [ 0.21214307  1.          0.14587678  0.14180858  0.18254738]\n",
      " [ 0.13333225  0.14587678  1.          0.05299842  0.10828555]\n",
      " [ 0.07074898  0.14180858  0.05299842  1.          0.05242471]\n",
      " [ 0.21040076  0.18254738  0.10828555  0.05242471  1.        ]]\n",
      "\n",
      "\n",
      "Top courses with query \"facebook\":\n",
      "   - Computational Social Media \t3.908\n",
      "Comparison matrix:\n",
      "[[ 1.]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query('markov chains')\n",
    "query('facebook')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
